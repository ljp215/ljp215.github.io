<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zane Blog</title>
  <icon>https://www.gravatar.com/avatar/96ec5919a1de173982539e20d0fb55da</icon>
  <subtitle>业精于勤荒于嬉，形成思毁于随</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://luojinping.com/"/>
  <updated>2019-08-25T13:10:11.000Z</updated>
  <id>http://luojinping.com/</id>
  
  <author>
    <name>Jinping Luo</name>
    <email>ljp215@foxmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习实战 — 支持向量机</title>
    <link href="http://luojinping.com/2019/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E2%80%94-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://luojinping.com/2019/03/04/机器学习实战-—-支持向量机/</id>
    <published>2019-03-04T15:23:56.000Z</published>
    <updated>2019-08-25T13:10:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h1><p>与「机器学习实战 — 决策树」的问题一样</p><h1 id="前情回顾"><a href="#前情回顾" class="headerlink" title="前情回顾"></a>前情回顾</h1><p>在上一次中，随机森林的效果最好，最终效果如下：<br><img src="/img/RF_finally_auc.jpg" alt="RF_finally_auc.jpg"></p><h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>特征工程与第二次作业一样，但增加了最后一步「特征归一化」，步骤概述如下：</p><ol><li>特征选取</li><li>删除无用特征</li><li>空值处理</li><li>处理重要特征</li><li>特征标签化</li><li><strong>特征归一化</strong></li></ol><h2 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h2><p>使用 StandardScaler 和 MinMaxScaler 的差别不大，最终使用的是 StandardScaler。因为特征基本上都是符合正态分布的，而且 StandardScaler 对数据变动时引入新的极值点更友好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 z-score 标准化特征</span></span><br><span class="line">ss = preprocessing.StandardScaler()</span><br><span class="line">X = ss.fit_transform(X)</span><br></pre></td></tr></table></figure><p>特征归一化的好处有以下两点：</p><ol><li>模型训练速度更快<br>能够使参数优化时能以较快的速度收敛。<br>归一化前后的 SVM(linear 核) 的耗时对比，数据集 (2783, 54)，即 2783 条数据，54 个特征。归一化前后的耗时分别为：1083s，0.44s，可见归一化对计算速度的提升非常大。</li><li>模型的准确率提升<br>将特征缩放到同一尺度的量级，能够使搜索的跳跃更加平滑，避免反复震荡的情况，提高准确率。可以参考下图形象化的解释：<br><img src="/img/features_normalization_gradient_descent.jpg" alt="features_normalization_gradient_descent.jpg"></li></ol><p>对于不同数量的训练集，训练 SVM，LR，DT，RF 四个模型。随着数据集的数量增加，特征归一化后的模型，其准确率提升如下图所示：<br><img src="/img/features_normalization_analysis.jpg" alt="features_normalization_analysis.jpg"></p><p>通过这个图，也会发现特征归一化也不是能够提升所有模型的准确率，对于 DT 和 RF 就没有效果，这是因为决策树的分支只是计算信息熵，而不考虑整体特征的分布情况。</p><h2 id="最终数据集"><a href="#最终数据集" class="headerlink" title="最终数据集"></a>最终数据集</h2><p>经过一系列处理后可用的数据集有 30w，但由于 SVM 运行地太慢了，从中选取 2w 数据来作为本次作业的数据集。</p><h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><h2 id="核函数介绍"><a href="#核函数介绍" class="headerlink" title="核函数介绍"></a>核函数介绍</h2><p>常见的核函数有：</p><ol><li>linear：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。</li><li>rbf：将样本映射到更高维的空间，目前应用最广的一个，对大小样本都有比较好的性能，而且参数较少。linear 是 rbf 的一个特例。</li><li>poly：多项式核函数，将低维的输入空间映射到高纬的特征空间，参数多，当多项式的阶数比较高时，计算复杂度会非常大。</li><li>sigmod：支持向量机实现的就是一种多层神经网络。</li></ol><p>Andrew Ng 的建议：</p><ol><li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li><li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li><li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li></ol><p>本次作业的数据集特征少，数据大。结合上述建议，再加上取了少部分(1k~5k)的数据进行了初步对比，决定重点调优 rbf 的参数。</p><h2 id="SVM-RBF-参数搜索"><a href="#SVM-RBF-参数搜索" class="headerlink" title="SVM RBF 参数搜索"></a>SVM RBF 参数搜索</h2><p>训练集数据量： 2w。搜索最优参数，用时 12.3 小时，将 搜索过程的数据记录 绘制成下图所示，纵轴代表搜索得分，横轴代表 {C, gamma} 两个参数的取值。</p><p><img src="/img/SVM_GridSearch_result.jpg" alt="SVM_GridSearch_result.jpg"></p><p>C = 10, gamma = 0.1 时的效果最好。如上图中红圈所示，对于 C = 0.1, 100, 1000 时，都是gamma = 0.1 这个位置时效果最好。</p><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p>在本次作业的数据集中，linear 核函数的运算速度相当慢，所以对于常用核函数，仅对比了 rbf，poly 和 sigmoid 三个核函数。针对 2w 条数据，运行结果如下：</p><p><img src="/img/SVM_normal_model_auc.jpg" alt="SVM_normal_model_auc.jpg"></p><p>其中，rbf 效果最好，AUC 有 0.82，但相比起之前 AUC 0.9 的 RF 来还是不理想。</p><h2 id="自定义核函数"><a href="#自定义核函数" class="headerlink" title="自定义核函数"></a>自定义核函数</h2><p>由于常用核函数的效果不够理想，所以尝试使用自定义的核函数，参考前人总结出的各种核函数，放入模型中进行尝试。核函数的公式见原代码，任取两个核函数的说明如下：</p><p><strong>Rational quadratic kernel</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Rational quadratic kernel, </span></span><br><span class="line"><span class="string">    K(x, y) = 1 - ||x-y||^2/(||x-y||^2+c)</span></span><br><span class="line"><span class="string">where:</span></span><br><span class="line"><span class="string">    c &gt; 0</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rational_quadratic</span><span class="params">(data_1, data_2)</span>:</span></span><br><span class="line">    _c = <span class="number">1</span></span><br><span class="line">    dists_sq = euclidean_dist_matrix(data_1, data_2)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> - (dists_sq / (dists_sq + _c))</span><br></pre></td></tr></table></figure></p><p><strong>Inverse multiquadratic kernel</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Inverse multiquadratic kernel, </span></span><br><span class="line"><span class="string">    K(x, y) = 1 / sqrt(||x-y||^2 + c^2)</span></span><br><span class="line"><span class="string">where:</span></span><br><span class="line"><span class="string">    c &gt; 0</span></span><br><span class="line"><span class="string">as defined in:</span></span><br><span class="line"><span class="string">"Interpolation of scattered data: Distance matrices and conditionally positive definite functions"</span></span><br><span class="line"><span class="string">Charles Micchelli</span></span><br><span class="line"><span class="string">Constructive Approximation</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inverse_multiquadratic</span><span class="params">(data_1, data_2)</span>:</span></span><br><span class="line">    _c = <span class="number">1</span> ** <span class="number">2</span></span><br><span class="line">    dists_sq = euclidean_dist_matrix(data_1, data_2)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / np.sqrt(dists_sq + _c)</span><br></pre></td></tr></table></figure></p><p>仍然是对于这 2w 条数据，将所有核函数放入 SVM 中训练，最终每个核函数的准确率和耗时对比表格如下：</p><p><img src="/img/SVM_model_compare.jpg" alt="SVM_model_compare.jpg"></p><p>可以发现 rbf, inverse_multiquadratic 和 cauchy 这三个核函数的效果较好，其中 rbf 训练速度最快，inverse_multiquadratic 准确率最高。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="SVM-准确率的思考"><a href="#SVM-准确率的思考" class="headerlink" title="SVM 准确率的思考"></a>SVM 准确率的思考</h2><p>最终，使用 inverse_multiquadratic 核函数可以将 SVM 模型的 AUC Score 最高调至 0.822，相比于第二次作业中 Random Forest 模型的 AUC Score 结果 0.902 还有差距。可能是由于核函数选取的仍然不够合适，在映射后的空间中数据不是那么线性可分，降低了模型的泛化能力，导致准确率不如 RF。而对于核函数的选取，需要更多地理解特征，并列举出所有可能的核函数，再进行对比选择，在选取核函数这一点上，没有很好的捷径可走。</p><h2 id="提升-SVM-训练速度的心得"><a href="#提升-SVM-训练速度的心得" class="headerlink" title="提升 SVM 训练速度的心得"></a>提升 SVM 训练速度的心得</h2><ol><li>特征标签化和归一化</li><li>SVC 的 cache_size 设置到 7000 (M) </li><li>核函数是 SVM 的关键，先用少部分数据来选核函数，再用全量数据训练</li><li>SVM 的 C 参数不要设置的太大</li></ol><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank" rel="noopener">1.4. Support Vector Machines — scikit-learn 0.20.1 documentation</a><br><a href="https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py" target="_blank" rel="noopener">RBF SVM parameters — scikit-learn 0.20.1 documentation</a><br><a href="https://www.jianshu.com/p/95e5faa3f709" target="_blank" rel="noopener">逻辑斯蒂回归VS决策树VS随机森林 - 简书</a><br><a href="https://www.csie.ntu.edu.tw/~r95162/guide.pdf" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~r95162/guide.pdf</a><br><a href="https://github.com/gmum/pykernels" target="_blank" rel="noopener">https://github.com/gmum/pykernels</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;要解决的问题&quot;&gt;&lt;a href=&quot;#要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;要解决的问题&quot;&gt;&lt;/a&gt;要解决的问题&lt;/h1&gt;&lt;p&gt;与「机器学习实战 — 决策树」的问题一样&lt;/p&gt;
&lt;h1 id=&quot;前情回顾&quot;&gt;&lt;a href=&quot;#前情回顾&quot;
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战 — 决策树</title>
    <link href="http://luojinping.com/2019/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E2%80%94-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://luojinping.com/2019/03/04/机器学习实战-—-决策树/</id>
    <published>2019-03-04T15:03:41.000Z</published>
    <updated>2019-08-25T13:10:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h1><p>一句话概述：<br>预测进入 App 首页的用户是否会点击「XLJH推荐模块」，该模块是 App 的一个功能，用户点击按钮则会进入一个XLJH的页面。</p><p>目前每天进入 App 首页的用户中，有 2.1234% 的用户会点击「XLJH推荐模块」，转化率非常低，而首页每日的曝光量是最大的，所以优化这个模块的转化率就变得尤为重要，也是本次作业要解决的问题。</p><h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>获取某一段时间的相关埋点事件、用户业务数据、用户画像等数据，将数据合并、清洗、整理为可用的数据集，然后跑决策树，随机森林和 Adaboost 三个模型。<br>要获取的数据是：</p><ul><li>用户被展示到该模块的埋点事件</li><li>用户点击该模块的埋点事件</li><li>用户使用过 XLJH 的统计数据</li><li>用户画像</li><li>用户的业务画像</li></ul><h1 id="特征选取"><a href="#特征选取" class="headerlink" title="特征选取"></a>特征选取</h1><h2 id="特征举例"><a href="#特征举例" class="headerlink" title="特征举例"></a>特征举例</h2><p>特征的总数非常多，总共有 280 个，抽取一些特征描述如下：</p><ul><li>年龄</li><li>性别</li><li>身高</li><li>体重</li><li>业务特征 1</li><li>业务特征 2</li><li>业务特征 3</li><li>… … </li><li>业务特征 n-1</li><li>业务特征 n</li><li>进入首页时距离上一次使用模块的时间</li><li>设备机型</li></ul><h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><p>通过在 hive 里跑 sql(具体的 sql 语句略) 获取到的数据量为：54w。（单位为数据行数，下同）</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="删除无用特征"><a href="#删除无用特征" class="headerlink" title="删除无用特征"></a>删除无用特征</h3><ol><li>无用特征的删除：例如 userId 等。这些特征明显与结果无关。</li><li>删除由预测结果导致的特征：例如使用过该模块的时间。这些特征是在预测值为 True 时才会有值，而且这特征的赋值在要预测的事件发生之后。</li></ol><h3 id="特征的空值处理"><a href="#特征的空值处理" class="headerlink" title="特征的空值处理"></a>特征的空值处理</h3><ol><li>删除特征为空的数据，例如年龄，性别等必须会有的特征。</li><li>int 类型的特征将空值填充为 0，例如某业务的 累计分钟数，某实体数量，粉丝数量，过去7天的XX业务使用统计等</li><li>string 类型的特征将空值填充为 0，例如 citycode，tags 等。</li></ol><h3 id="处理重要特征"><a href="#处理重要特征" class="headerlink" title="处理重要特征"></a>处理重要特征</h3><p>跑完 Random Forest 模型，可以输出 Feature importances 列表， Top 10 的图略。</p><p>其中需要特别处理的特征是 c.bmi 和 c.age。<br>分析数据后发现年龄为空值的情况非常少，所以把 c.age 为空的数据删掉。<br>c.bmi 需要用户填入身高和体重，这部分数据缺失一些，所以填为 c.bmi 这一列的平均值，数据集中的 dataset[c.bmi].mean = 23.3174390247965，符合常识。</p><h3 id="特征标签化"><a href="#特征标签化" class="headerlink" title="特征标签化"></a>特征标签化</h3><p>数据标准化的处理方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">le = preprocessing.LabelEncoder()</span><br><span class="line">dataset = dataset.apply(le.fit_transform)</span><br></pre></td></tr></table></figure><h3 id="可用数据量"><a href="#可用数据量" class="headerlink" title="可用数据量"></a>可用数据量</h3><p>按如上方式处理完后，最终可用的数据量为：30w</p><h1 id="模型调优"><a href="#模型调优" class="headerlink" title="模型调优"></a>模型调优</h1><h2 id="Decision-Tree-调优"><a href="#Decision-Tree-调优" class="headerlink" title="Decision Tree 调优"></a>Decision Tree 调优</h2><h3 id="树的深度调优"><a href="#树的深度调优" class="headerlink" title="树的深度调优"></a>树的深度调优</h3><p>使用的特征数量为 275 个，使用的训练集数量为 54801。</p><p>不同树深度对应的模型评估如下：</p><p><img src="/img/tree_depth_3_to_5.jpg" alt="tree_depth_3_to_5.jpg"></p><p><img src="/img/tree_depth_10_to_50.jpg" alt="tree_depth_10_to_50.jpg"></p><p>可以发现树的深度越高，召回率越高，准确率却越低。总体评价是树的深度为 5 最好。</p><h3 id="其他参数调优"><a href="#其他参数调优" class="headerlink" title="其他参数调优"></a>其他参数调优</h3><p>max_features=’sqrt’ 加了这个反而变差了很多。<br>min_samples_split=5, min_samples_leaf=5，加了这个反而变差了一些。</p><h3 id="DT-输出"><a href="#DT-输出" class="headerlink" title="DT 输出"></a>DT 输出</h3><p>深度为 5 的 DT 截取部分放大后的图如下：<br><img src="/img/tree_depth_5_img.jpg" alt="tree_depth_5_img.jpg"></p><h2 id="Random-Forest-调优"><a href="#Random-Forest-调优" class="headerlink" title="Random Forest 调优"></a>Random Forest 调优</h2><p>n_estimators：100, 300, 500 都试过，差别不大。弱学习器的最大迭代次数太小会不准确，太大模型训练地就很慢。<br>oob_score: True，即采用袋外样本来评估模型的好坏，提高模型拟合后的泛化能力。<br>基本上，RF 不怎么需要调参。</p><p>比较有价值的是能产生特征的重要性，Top 30 Feature importances 的图片略。<br>Feature importances Top 10 的可视化略。</p><p>使用 GridSearchCV 搜索最优参数，参数搜索结果如下：<br><img src="/img/GridSearchCV_result.jpg" alt="GridSearchCV_result.jpg"></p><p>但是使用参数后，RF 的 AUC 降到了 0.82，应该是 max_features 和 min_samples_leaf 这两个参数调的不对。</p><h2 id="Adaboost-调优"><a href="#Adaboost-调优" class="headerlink" title="Adaboost 调优"></a>Adaboost 调优</h2><p>AdaBoostClassifier 的 base_estimator 选择已经之前训练好的 DT，效果更好，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">adb_clf = AdaBoostClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">100</span>, learning_rate=<span class="number">0.02</span>).fit(X_train, y_train)</span><br><span class="line">AUC Score (Train): <span class="number">0.629162</span></span><br><span class="line">adb_clf_2 = AdaBoostClassifier(clf, n_estimators=<span class="number">100</span>, random_state=<span class="number">100</span>, learning_rate=<span class="number">0.02</span>).fit(X_train, y_train)</span><br><span class="line">AUC Score (Train): <span class="number">0.659129</span></span><br></pre></td></tr></table></figure><p>learning_rate 设置为 0.02 能兼顾速度和效果。</p><h1 id="踩过的坑"><a href="#踩过的坑" class="headerlink" title="踩过的坑"></a>踩过的坑</h1><h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>基尼指数和信息熵的几乎无差别，不怎么影响结果。</p><h2 id="PCA-降维"><a href="#PCA-降维" class="headerlink" title="PCA 降维"></a>PCA 降维</h2><p>PCA 降维，使用 mle 和 5 个特征，都不好使，准确度反而更低，AUC 得分降至 0.51。</p><h2 id="用了未来特征"><a href="#用了未来特征" class="headerlink" title="用了未来特征"></a>用了未来特征</h2><p>决策树 AUC: 0.52 -&gt; 0.77<br>随机森林 AUC: 0.82-&gt;0.93</p><p>随机森林的模型评价如下：<br><img src="/img/RF_auc.jpg" alt="RF_auc.jpg"></p><p>会发现准确度太高了，仔细分析后，发现用了一个「创建体测时间」的特征，而这个特征是在用户点击了 「智能计划推荐模块」后会更新值的，所以相当于用了事件发生后的特征来预测事件发生的概率，这样肯定会导致模型的准确率很高。后来去掉了这个特征。</p><h1 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h1><p>数据集：30w，其中正样本的比例为 22%。</p><p><img src="/img/dt_model_compare.jpg" alt="dt_model_compare.jpg"></p><p>对比发现，DT 和 Adaboost 的效果都不好，AUC Score 都在 0.65 左右。RF 的效果最好，AUC Score 能到 0.90。<br>可能是因为数据集符合 low bias, high variance 的规律，所以 RF 要比 Adaboost 好。</p><p>RF 的准确率和召回率都还不错，感觉可以后续上线用于 App 首页了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整个过程可以抽象为：<br>获取数据 -&gt; 调整模型参数 -&gt; 引入更多特征 -&gt; 调整模型参数 -&gt; PCA -&gt; 减少特征 -&gt; 处理重要特征 -&gt; gridSearchCV -&gt; 调整模型参数</p><p>一些总结：</p><ol><li>特征工程很重要，特征处理好后 AUC 有明显的提升</li><li>调参也有用，但相比起来，好的特征更有用</li><li>基尼系数和熵的区别不大</li><li>树的层数越多，准确率越低，召回越高</li><li>AdaBoostClassifier base_estimators 用训练好的决策树来做，效果更好 </li><li>gridSearchCV 搜索最有用的参数太慢了，而且最终效果还不好</li><li>小心引入未来特征！</li></ol><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://blog.csdn.net/qq_29003925/article/details/75222560" target="_blank" rel="noopener">决策树分类器在Scikit-learn的使用小结 - qq_29003925的博客 - CSDN博客</a><br><a href="https://blog.csdn.net/FontThrone/article/details/78824945" target="_blank" rel="noopener">sklearn中的回归决策树 - FontTian的专栏 - CSDN博客</a><br><a href="http://sklearn.apachecn.org/cn/latest/modules/tree.html" target="_blank" rel="noopener">1.10. 决策树 — scikit-learn 0.19.0 中文文档 - ApacheCN</a><br><a href="https://blog.csdn.net/sinat_29957455/article/details/78997244" target="_blank" rel="noopener">pandas的汇总和计算描述统计 - 修炼之路 - CSDN博客</a><br><a href="https://www.jianshu.com/p/8ba8744b4c48" target="_blank" rel="noopener">数据分析-pandas数据处理清洗常用总结 - 简书</a><br><a href="https://www.zhihu.com/question/29316149/answer/252391239" target="_blank" rel="noopener">https://www.zhihu.com/question/29316149/answer/252391239</a><br><a href="https://blog.csdn.net/m0_37324740/article/details/81431430" target="_blank" rel="noopener">集成学习概述（Bagging，RF，GBDT，Adaboost） - U R MINE - CSDN博客</a><br><a href="https://blog.csdn.net/u010089444/article/details/70053104" target="_blank" rel="noopener">数据预处理与特征选择 - Joe的博客 - CSDN博客</a><br><a href="http://www.cnblogs.com/robert-dlut/p/5276927.html" target="_blank" rel="noopener">谈谈评价指标中的宏平均和微平均 - robert_ai - 博客园</a><br><a href="https://blog.csdn.net/sinat_26917383/article/details/75199996" target="_blank" rel="noopener">https://blog.csdn.net/sinat_26917383/article/details/75199996</a><br><a href="https://blog.csdn.net/akon_wang_hkbu/article/details/77621631" target="_blank" rel="noopener">DecisionTreeClassifier和DecisionTreeClassifier 重要参数调参注意点 - akon_wang_hkbu的博客 - CSDN博客</a><br><a href="https://www.cnblogs.com/songxingzhu/p/6001373.html" target="_blank" rel="noopener">机器学习-分类器-Adaboost原理 - 宋兴柱 - 博客园</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;要解决的问题&quot;&gt;&lt;a href=&quot;#要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;要解决的问题&quot;&gt;&lt;/a&gt;要解决的问题&lt;/h1&gt;&lt;p&gt;一句话概述：&lt;br&gt;预测进入 App 首页的用户是否会点击「XLJH推荐模块」，该模块是 App 的一个功能
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战 — 朴素贝叶斯</title>
    <link href="http://luojinping.com/2019/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E2%80%94-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://luojinping.com/2019/03/04/机器学习实践-—-朴素贝叶斯/</id>
    <published>2019-03-04T14:26:47.000Z</published>
    <updated>2019-08-25T13:10:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h1><p>根据给定的 20-News Group 数据集，设计Naïve Bayes 算法进行文本分类的研究。<br>讨论和使用不同的预处理方法，并讨论各种预处理对于算法性能的影响。</p><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><ol><li>加载数据集</li><li>提取  TF-IDF 特征</li><li>生成文档的 TF-IDF 矩阵</li><li>训练多项式朴素贝叶斯模型</li><li>评价模型</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn twenty_newsgroups document</span></span><br><span class="line"><span class="comment"># http://sklearn.apachecn.org/cn/0.19.0/datasets/twenty_newsgroups.html</span></span><br><span class="line">fetch_20newsgroups(data_home=<span class="string">'/Users/ljp/Codes/ivy_plan/naive_bayes/20_newsgroups'</span>, <span class="comment"># 文件下载的路径</span></span><br><span class="line">                   subset=<span class="string">'train'</span>, <span class="comment"># 加载那一部分数据集 train/test</span></span><br><span class="line">                   categories=<span class="literal">None</span>, <span class="comment"># 选取哪一类数据集[类别列表]，默认20类</span></span><br><span class="line">                   shuffle=<span class="literal">True</span>,  <span class="comment"># 将数据集随机排序</span></span><br><span class="line">                   random_state=<span class="number">42</span>, <span class="comment"># 随机数生成器</span></span><br><span class="line">                   remove=(), <span class="comment"># ('headers','footers','quotes') 去除部分文本</span></span><br><span class="line">                   download_if_missing=<span class="literal">True</span> <span class="comment"># 如果没有下载过，重新下载</span></span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">newsgroups_train = fetch_20newsgroups(subset=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bunch</span></span><br><span class="line"><span class="comment"># load_*和 fetch_* 函数返回的数据类型是 datasets.base.Bunch，本质上是一个 dict，它的键值对可用通过对象的属性方式访问。</span></span><br><span class="line"><span class="comment"># 主要包含以下属性：</span></span><br><span class="line"><span class="comment"># data：特征数据数组，type 是 list，是 n_samples * n_features 的二维 numpy.ndarray 数组</span></span><br><span class="line"><span class="comment"># filenames：文件数组，是文件路径，是 n_samples 的一维 numpy.ndarray 数组</span></span><br><span class="line"><span class="comment"># target：标签数组，是类别的整数索引，是 n_samples 的一维 numpy.ndarray 数组，与 filenames 一一对应</span></span><br><span class="line"><span class="comment"># DESCR：数据描述</span></span><br><span class="line"><span class="comment"># feature_names：特征名</span></span><br><span class="line"><span class="comment"># target_names：标签名</span></span><br><span class="line"><span class="comment"># print(type(newsgroups_train)) # &lt;class 'sklearn.utils.Bunch'&gt;</span></span><br><span class="line"><span class="comment"># print(list(newsgroups_train)) # ['data', 'filenames', 'target_names', 'target', 'DESCR', 'description']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取tfidf特征</span></span><br><span class="line">vectorizer = TfidfVectorizer() <span class="comment"># vectorizer type is &lt;class 'sklearn.feature_extraction.text.TfidfVectorizer'&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vectors type is &lt;class 'scipy.sparse.csr.csr_matrix'&gt;</span></span><br><span class="line"><span class="comment"># 数据格式为：(文档序号, 单词序号), tf-idf值</span></span><br><span class="line"><span class="comment"># 文档序号是该文档在 newsgroups_train.filenames 里的索引(位置)</span></span><br><span class="line"><span class="comment"># 单词序号是该单词在所有单词所构成的一维向量(也称为词袋，Bag of words)里的索引(位置)</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="comment">#  (0, 11071)0.02879840104494835</span></span><br><span class="line"><span class="comment">#  (0, 19516)0.15199951710440102</span></span><br><span class="line"><span class="comment">#  (0, 24556)0.36446543134314724</span></span><br><span class="line">vectors = vectorizer.fit_transform(newsgroups_train.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MultinomialNB实现文本分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score,f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">clf=MultinomialNB(alpha=<span class="number">0.1</span>)</span><br><span class="line">clf.fit(vectors,newsgroups_train.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试集</span></span><br><span class="line">newsgroups_test=fetch_20newsgroups(subset=<span class="string">'test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取测试集tfidf特征</span></span><br><span class="line">vectors_test=vectorizer.transform(newsgroups_test.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">pred=clf.predict(vectors_test)</span><br><span class="line">print(<span class="string">'accuracy_score: '</span> + str(accuracy_score(newsgroups_test.target,pred)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"classification report on test set for classifier:"</span>)</span><br><span class="line">print(clf)</span><br><span class="line">X_test = vectorizer.transform((d <span class="keyword">for</span> d <span class="keyword">in</span> newsgroups_test.data))</span><br><span class="line">pred = clf.predict(X_test)</span><br><span class="line">y_test = newsgroups_test.target</span><br><span class="line">print(classification_report(y_test, pred, target_names=newsgroups_test.target_names))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成混淆矩阵，观察每种类别被错误分类的情况</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, pred)</span><br><span class="line">print(<span class="string">"confusion matrix:"</span>)</span><br><span class="line">print(cm)</span><br><span class="line"><span class="comment"># Show confusion matrix</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>), dpi=<span class="number">144</span>)</span><br><span class="line">plt.title(<span class="string">'Confusion matrix of the classifier'</span>)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.set_xticklabels([])</span><br><span class="line">ax.set_yticklabels([])</span><br><span class="line">plt.matshow(cm, fignum=<span class="number">1</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输出如下：<br><img src="/img/naive_bayes_auc.jpg" alt="naive_bayes_auc.jpg"></p><p><img src="/img/naive_bayes_cross_matrix.png" alt="naive_bayes_cross_matrix.png"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="http://sklearn.apachecn.org/cn/0.19.0/datasets/twenty_newsgroups.html" target="_blank" rel="noopener">5.6.2. 20个新闻组文本数据集 — scikit-learn 0.19.0 中文文档 - ApacheCN</a><br><a href="https://www.jianshu.com/p/364887de2039" target="_blank" rel="noopener">利用朴素贝叶斯算法进行文档分类 - 简书</a><br><a href="https://www.cs.waikato.ac.nz/ml/publications/2004/kibriya_et_al_cr.pdf" target="_blank" rel="noopener">https://www.cs.waikato.ac.nz/ml/publications/2004/kibriya_et_al_cr.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;作业&quot;&gt;&lt;a href=&quot;#作业&quot; class=&quot;headerlink&quot; title=&quot;作业&quot;&gt;&lt;/a&gt;作业&lt;/h1&gt;&lt;p&gt;根据给定的 20-News Group 数据集，设计Naïve Bayes 算法进行文本分类的研究。&lt;br&gt;讨论和使用不同的预处理方法，并讨
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>线性回归正则化</title>
    <link href="http://luojinping.com/2018/11/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://luojinping.com/2018/11/11/线性回归正则化/</id>
    <published>2018-11-11T13:56:11.000Z</published>
    <updated>2019-08-25T13:10:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>线性回归出的模型如果出现过拟合怎么办？</p><ol><li>脏数据太多，需要清洗数据</li><li>增加训练数据的数量和多样性</li><li>特征数量过多，使用正则化减少特征数量</li></ol><h1 id="向量的范数"><a href="#向量的范数" class="headerlink" title="向量的范数"></a>向量的范数</h1><p>向量的范数是一种用来刻画向量大小的一种度量。实数的绝对值，复数的模，三维空间向量的长度，都是抽象范数概念的原型。上述三个对象统一记为 x ，衡量它们大小的量记为  ||x|| （我们用单竖线表示绝对值，双竖线表示范数），显然它们满足以下三条性质：</p><p>L0范数：向量中非零元素的个数。<br>L1范数：向量中各个元素绝对值之和，又叫“稀疏规则算子”（Lasso regularization）<br>L2范数：向量中各个元素平方和再开方<br>p-范数：$||\textbf{x}||_p = (\sum_{i=1}^N|x_i|^p)^{\frac{1}{p}}$，即向量元素绝对值的p次方和的1/p次幂。</p><p>下图展示了 p 取不同值时 unit ball 的形状：</p><p><img src="/img/kinds_of_norm_graph.png" alt></p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>在统计学的缩减中，引入了惩罚项，减少了不重要的参数，同时还可采用正则化（regularization）减少不重要的参数。<br>既然是减少特征，那么最容易想到的就是使用 L0 范数，求回归函数中的参数向量 w 的非零元素的个数。如果约束 $‖w‖_0≤k$，就是约束非零元素个数不大于 k。不过很明显，L0 范数是不连续的且非凸的，如果在线性回归中加上 L0 范数的约束，就变成了一个组合优化问题：挑出 $≤k$ 个系数然后做回归，找到目标函数的最小值对应的系数组合，这是一个 NP 问题。</p><p>有趣的是，L1 范数也可以达到稀疏的效果，是 L0 范数的最优凸近似。我们把引入 L1 范数的线性回归叫做 Lasso 回归。</p><h2 id="Lasso-回归"><a href="#Lasso-回归" class="headerlink" title="Lasso 回归"></a>Lasso 回归</h2><p>Lasso 算法（英语：least absolute shrinkage and selection operator，又译最小绝对值收敛和选择算子、套索算法）是一种同时进行特征选择和正则化（数学）的回归分析方法，旨在增强统计模型的预测准确性和可解释性，最初由斯坦福大学统计学教授 Robert Tibshirani 于 1996 年基于 Leo Breiman 的非负参数推断(Nonnegative Garrote, NNG)提出。</p><p>优化目标：min $ 1/N\ast\sum_{i = 1}^{N}{(y_{i} -\omega^{T} x_{i})^{2} }$</p><p>Lasso 回归：min  $1/N\ast\sum_{i = 1}^{N}{(y_{i} -\omega^{T} x_{i})^{2} } + \lambda||\omega||_{1}$</p><h2 id="Ridge-回归"><a href="#Ridge-回归" class="headerlink" title="Ridge 回归"></a>Ridge 回归</h2><p>岭回归是加了二阶正则项的最小二乘，主要适用于过拟合严重或各变量之间存在多重共线性的时候，岭回归是有 bias 的，这里的 bias 是为了让 variance 更小。</p><p>Ridge 回归：min  $1/N\ast\sum_{i = 1}^{N}{(y_{i} -\omega^{T} x_{i})^{2} } + \lambda ||\omega||_{2}^{2} $</p><p>岭回归最先是用来处理特征数多与样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里引入λ限制了所有w的和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学上也叫做缩减。缩减方法可以去掉不重要的参数，因此能更好的理解数据。选取不同的λ进行测试，最后得到一个使得误差最小λ。</p><p>缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。</p><h2 id="比较两者"><a href="#比较两者" class="headerlink" title="比较两者"></a>比较两者</h2><p>Lasso 回归与 Ridge 回归有共同点，也有区别。</p><h3 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h3><p>都能解决两个问题：</p><ol><li>线性回归出现的过拟合现象</li><li>使用 Normal equation 求解时，解决 $(X^TX)$ 不可逆的问题。</li></ol><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>岭回归加入的正则项是 L2 范数，其结果可以将偏回归系数往 0 的方向进行压缩，但不会把偏回归系数压缩为 0，即岭回归不会剔除变量。Lasso 回归同样也可以将偏回归系数往 0 方向压缩，但是能够将某些变量的偏回归系数压缩为 0，因此可以起到变量筛选的作用。</p><p><img src="/img/ridege_and_lasso_solution.png" alt></p><p>红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么 L1 范式会是稀疏的。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">Lasso算法 - 维基百科，自由的百科全书</a><br><a href="https://blog.csdn.net/xbinworld/article/details/44276389" target="_blank" rel="noopener">机器学习方法：回归（二）：稀疏与正则约束ridge regression，Lasso</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;线性回归出的模型如果出现过拟合怎么办？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;脏数据太多，需要清洗数据&lt;/li&gt;
&lt;li&gt;增加训练数据的数量和多样性&lt;/
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 7 SVM</title>
    <link href="http://luojinping.com/2018/04/14/Stanford-Machine-Learning-7-SVM/"/>
    <id>http://luojinping.com/2018/04/14/Stanford-Machine-Learning-7-SVM/</id>
    <published>2018-04-14T10:33:50.000Z</published>
    <updated>2019-08-25T13:13:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SVM-定义"><a href="#SVM-定义" class="headerlink" title="SVM 定义"></a>SVM 定义</h1><p>支持向量机，因其英文名为 support vector machine，故一般简称 SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p><h1 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h1><p><img src="/img/svm_margin.jpg" alt="svm_margin"><br>上图中哪个分类器最好呢？</p><h2 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h2><p>为了方便介绍 SVM，重新定义以下符号。</p><ol><li>在logstic回归中我们用0,1代表两个类, 现在我们改用-1,+1, 即 y∈{-1，-1}</li><li>在logistic回归中, 我们的 g 是sigmoid函数, 现在改为:<br>g(z)=1, z&gt;=0<br>g(z)=-1, z&lt;0</li><li>在logistic回归中, 我们的假设函数为 $h_θ(x)$, 现在改为,  <script type="math/tex">h_\theta(x)</script><br>$h_{x,b}(x)=g(w^{T}x+b)$, 其中w相当于$[θ_1,θ_2,θ_3,…θ_n]^T$, b 相当于 $θ_0$，即截距。</li></ol><p>h_{x,b}</p><script type="math/tex; mode=display">x=a+b</script><h2 id="函数间隔-functional-margin"><a href="#函数间隔-functional-margin" class="headerlink" title="函数间隔(functional margin)"></a>函数间隔(functional margin)</h2><p>对于一个训练样本 $(x^{(i)}, y^{(i)})$，我们定义它到超平面 (w,b) 的函数间隔为:</p><script type="math/tex; mode=display">\widehat\gamma=y^{(i)}(w^{T}x^{(i)}+b)</script><p>我们希望函数间隔越大越好, 即:<br>if $y^{(i)}=1$, want $(w^{T}x^{(i)}+b) \gg 0 $ (&gt;&gt;代表远大于)<br>if $y^{(i)}=-1$, want $(w^{T}x^{(i)}+b) \ll 0$</p><p>函数间隔越大，代表我们对于分类的结果非常确定。</p><p>但这里有一个漏洞，即可以在不改变这个超平面的情况下可以让函数间隔任意大，只要我们成比增加 w,b 就可以达到这个目的了。例如，我们将 w 变为 2w， b 变为 2b，那么我们的函数间隔将会是原来的两倍。</p><p>所以对于整个训练集, <strong>函数间隔</strong>定义为：</p><script type="math/tex; mode=display">\widehat\gamma=min{\widehat\gamma^{(i)}}</script><h2 id="几何间隔-geometric-margin"><a href="#几何间隔-geometric-margin" class="headerlink" title="几何间隔(geometric margin)"></a>几何间隔(geometric margin)</h2><p>定义对于一个训练样本 $A(x^{(i)}, y^{(i)})$，它到超平面 $w^{T}x^{(i)}+b$ 的几何距离为 $\gamma^{(i)}$。<br>设 B 为 A 在超平面上的投影，易得超平面的法向量为 $\frac{w}{||w||}$，则有 $A=B+\gamma^{(i)}\frac{w}{||w||}$，<br>即 $B = A - \gamma^{(i)}\frac{w}{||w||}$。又因为 B 在超平面上，所以有<br>$w^{T}(x^{(i)} - \gamma^{(i)}\frac{w}{||w||})+b=0$<br>故几何距离为:</p><script type="math/tex; mode=display">\gamma^{(i)}=(\frac{w}{||w||})^{T}x^{(i)} + \frac{b}{||w||}</script><p>定义其几何间隔：</p><script type="math/tex; mode=display">\gamma^{(i)}=y^{(i)}[(\frac{w}{||w||})^{T}x^{(i)} + \frac{b}{||w||}]</script><p>所以对于整个训练集, <strong>几何间隔</strong>定义为：</p><script type="math/tex; mode=display">\gamma=min{\gamma^{(i)}}</script><p>可以发现，当 $||w||=1$时，$\widehat\gamma^{(i)}=\gamma^{(i)}$</p><h1 id="最优间隔分类器"><a href="#最优间隔分类器" class="headerlink" title="最优间隔分类器"></a>最优间隔分类器</h1><p>几何间隔就是在求 $\frac{\widehat\gamma}{||w||}$ 的最小值，可以发现函数间隔 $\widehat\gamma$ 可放大缩小，且其对结果不产生影响，所以不妨设令${\widehat\gamma}=1$。<br>现在，目标函数转为了：</p><script type="math/tex; mode=display">max \frac{1}{||w||}, s.t., y^{(i)}(w^{T}x^{(i)}+b)\ge1, i=1,2,3...,n</script><p>等价于</p><script type="math/tex; mode=display">min{\frac12{||w||^2}}, s.t., y^{(i)}(w^{T}x^{(i)}+b)\ge1, i=1,2,3...,n</script><p>利用拉格朗日乘子法可得：</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac12{||w||^2}-\sum_{i=1}^{n}\alpha_i[y^{(i)}(w^{T}x^{(i)}+b)-1]</script><p>令</p><script type="math/tex; mode=display">\theta(w)=\displaystyle\max_{\alpha_i\ge0}L(w, b, \alpha)</script><p>则目标函数变成了：</p><script type="math/tex; mode=display">\displaystyle\min_{w,b}\theta(w)=\displaystyle\min_{w,b}\max_{\alpha_i\ge0}L(w, b, \alpha)=p^\*</script><h1 id="求解目标函数"><a href="#求解目标函数" class="headerlink" title="求解目标函数"></a>求解目标函数</h1><h2 id="使用对偶问题求解"><a href="#使用对偶问题求解" class="headerlink" title="使用对偶问题求解"></a>使用对偶问题求解</h2><p>SVM 中用到了高维映射，将线性不可分的问题映射为线性可分，且映射函数的具体形式无法提前确定，而往往使用核函数映射后，维度 w 会提升很多，甚至至无穷维。<br>在原问题下，求解算法的复杂度与样本维度（w 的维度）相关；<br>在对偶问题下，求解算法的复杂度与样本数量（等于拉格朗日算子 a 的数量）相关。</p><p>因此，如果是做线性分类，且样本维度低于样本数量的话，可以在原问题下求解。例如 Liblinear 的线性 SVM 默认做法就是这样的；<br>但如果是做非线性分类，那就会涉及到升维（比如使用高斯核做核函数，其实是将样本升到无穷维），升维后的样本维度往往会远大于样本数量，此时显然在对偶问题下求解会更好。</p><p><strong>直接求解原问题有多难？</strong> TBD</p><h3 id="使用对偶问题的解法"><a href="#使用对偶问题的解法" class="headerlink" title="使用对偶问题的解法"></a>使用对偶问题的解法</h3><p>对于不等书约束条件的最优化问题，使用拉格朗日对偶问题来求解。具体介绍见之前的 blog:  <a href="http://luojinping.com/2018/03/04/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/">拉格朗日乘子法</a></p><p>用 $p^*$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。<br>不过，把最小和最大的位置交换一下：</p><script type="math/tex; mode=display">\displaystyle\max_{\alpha_i\ge0}\min_{w,b}L(w, b, \alpha)=d^*</script><p>交换以后的问题不再等价于原问题，新问题的最优值用 $d^<em>$ 来表示。并且，有 $d^</em>$ ≤ $p^*$。</p><p>第二个问题的最优值 $d^<em>$ 提供了一个第一个问题的最优<br>值 $p^</em>$ 的一个下界。经过论证，原始问题满足强对偶所需要的条件，故这两者相等，所以可以通过求解第二个问题来间接地求解第一个问题。 </p><h3 id="优化公式"><a href="#优化公式" class="headerlink" title="优化公式"></a>优化公式</h3><p>要让 L 关于 w 和 b 最小化，分别对 w，b 求偏导数，即令<br>$\frac{∂L}{∂w}$ 和 $\frac{∂L}{∂b}$ 等于零，有：</p><script type="math/tex; mode=display">\frac{∂L}{∂w}=w-\sum_{i=1}^{n}\alpha_iy^{(i)}x^{(i)}=0</script><script type="math/tex; mode=display">\frac{∂L}{∂b}=\sum_{i=1}^{n}\alpha_iy^{(i)}=0</script><p>将上式代入：</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac12{||w||^2}-\sum_{i=1}^{n}\alpha_i[y^{(i)}(w^{T}x^{(i)}+b)-1]</script><p>推导过程如下：<br><img src="/img/svm_lagrange_dual_1.jpg" alt="svm_lagrange_dual_1"></p><p>这样求出 $\alpha$ 后即可得到 w 和 b。</p><p><img src="/img/svm_lagrange_dual_2.jpg" alt="svm_lagrange_dual_2"></p><p>现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。我们通过这个优化算法能得到α，再根据α，我们就可以求解出w和b，进而求得我们最初的目的：找到超平面，即”决策平面”。</p><h2 id="SMO-算法"><a href="#SMO-算法" class="headerlink" title="SMO 算法"></a>SMO 算法</h2><p><a href="https://zhuanlan.zhihu.com/p/29212107" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29212107</a><br><a href="https://cloud.tencent.com/developer/article/1076970" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1076970</a> </p><p><img src="/img/svm_smo_1.jpg" alt="SMO_JP_1.jpeg"><br><img src="/img/svm_smo_2.jpg" alt="SMO_JP_2.jpeg"></p><h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1><p>如果数据集就是线性不可分的应该怎么处理呢？处理方法是将数据集映射到更高维的空间，变成线性可分的。如下图所示：<br><img src="/img/svm_kernel_demo.jpg" alt="svm_kernel_demo"></p><p>一般使用高斯核，但这样会导致映射后的维度非常巨大，也就是 w 的维度很大，这也是为什么要转化为对偶问题来求解的原因，对偶问题的时间复杂度只和数据集的数量有关，与维度无关。</p><h1 id="总结-SVM"><a href="#总结-SVM" class="headerlink" title="总结 SVM"></a>总结 SVM</h1><p>SVM 是神经网络出现之前最好的分类算法。求解 SVM 的过程也就是找到区分正负数据的最优超平面，所以引入了几何间隔的概念。而求解最大的几何间隔的问题即是在不等式约束条件下求解最优解的问题。这就引入了拉格朗日对偶问题，接着针对对偶问题求解，引入快速学习算法 SMO，最终找到超平面。<br>对于原始数据线性不可分的情况，引入核函数映射到高维计算，这其中 SVM 求解过程的时间复杂度与维度无关。</p><p>附一个很精髓的 <a href="http://guoze.me/2014/11/26/svm-knowledge/" target="_blank" rel="noopener">SVM十问十答</a>。</p><h1 id="SVM-优缺点"><a href="#SVM-优缺点" class="headerlink" title="SVM 优缺点"></a>SVM 优缺点</h1><p>优点：</p><ol><li>可用于线性/非线性分类</li><li>可以解决高维问题，即大型特征空间;</li><li>泛化错误率低</li><li>结果容易解释</li><li>可以避免神经网络结构选择和局部极小点问题</li><li>SVM 尽量保持与样本间距离的性质导致它抗攻击的能力更强</li></ol><p>缺点：</p><ol><li>对参数调节和和函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题</li><li>在大规模训练样本下，效率不高</li><li>对非线性问题有时很难找到一个合适的核函数</li><li>解决多分类问题存在困难 </li></ol><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="点到平面的距离"><a href="#点到平面的距离" class="headerlink" title="点到平面的距离"></a>点到平面的距离</h2><p><img src="/img/svm_distance.jpg" alt="IMAGE"></p><p>$d$ 维空间中的超平面由下面的方程确定: $w^Tx+b=0$，其中，$w$ 与 $x$ 都是 $d$ 维列向量， $x=(x_1,x_2,…,x_d)^T$ 为平面上的点， $w=(w_1,w_2,…,w_d)^T$为平面的法向量。$b$ 是一个实数， 代表平面与原点之间的距离。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p><a href="http://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="noopener">http://blog.csdn.net/v_july_v/article/details/7624837</a><br><a href="http://guoze.me/2014/11/26/svm-knowledge/" target="_blank" rel="noopener">http://guoze.me/2014/11/26/svm-knowledge/</a><br><a href="https://pdfs.semanticscholar.org/59ee/e096b49d66f39891eb88a6c84cc89acba12d.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/59ee/e096b49d66f39891eb88a6c84cc89acba12d.pdf</a><br><a href="http://luojinping.com/2018/03/04/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/">http://luojinping.com/2018/03/04/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SVM-定义&quot;&gt;&lt;a href=&quot;#SVM-定义&quot; class=&quot;headerlink&quot; title=&quot;SVM 定义&quot;&gt;&lt;/a&gt;SVM 定义&lt;/h1&gt;&lt;p&gt;支持向量机，因其英文名为 support vector machine，故一般简称 SVM，通俗来讲，它是一
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日乘子法</title>
    <link href="http://luojinping.com/2018/03/04/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/"/>
    <id>http://luojinping.com/2018/03/04/拉格朗日乘子法/</id>
    <published>2018-03-04T12:23:40.000Z</published>
    <updated>2019-08-25T13:35:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在数学中的最优化问题中，拉格朗日乘数法（以数学家约瑟夫·拉格朗日命名）是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。这种方法可以将一个有n个变量与k个约束条件的最优化问题转换为一个解有n + k个变量的方程组的解的问题。这种方法中引入了一个或一组新的未知数，即拉格朗日乘数，又称拉格朗日乘子，或拉氏乘子，它们是在转换后的方程，即约束方程中作为梯度（gradient）的线性组合中各个向量的系数。</p><p>比如，要求 $f(x,y) 在 g(x,y)=c$ 时的最大值时，我们可以引入新变量拉格朗日乘数 $L(x,y,λ)=f(x,y) + λ(g(x,y)-c)$，更一般地，对含n个变量和k个约束的情况，有：</p><script type="math/tex; mode=display">L(x_1,x_2, ..., x_n, λ_1,λ_2,...,λ_n)=f(x_1,x_2, ..., x_n) - \sum_{i=1}^{k}λ_ig_i(x_1,x_2, ..., x_n)</script><p>拉格朗日乘数法所得的极点会包含原问题的所有极值点，但并不保证每个极值点都是原问题的极值点。拉格朗日乘数法的正确性的证明牵涉到偏微分，全微分或链法。</p><h2 id="用处"><a href="#用处" class="headerlink" title="用处"></a>用处</h2><p>现要解决以下问题，在满足 g(x,y)=c 这个等式的前提下，求 f(x,y) 函数的最小值（最大值道理相同）。这样的问题我们在高中的时候就遇到过了，只不过高中时遇到的限制条件 g(x,y)=c 都比较简单，一般而言都可以将 y 用 x 的式子表示出来，然后用变量替换的方法代回 f(x,y) 求解。但是，如果 g(x,y) 的形式过于复杂，或者变量太多时，这种方法就失效了。而拉格朗日乘子法就是解决这类问题的通用策略。</p><h1 id="等式约束条件的优化问题"><a href="#等式约束条件的优化问题" class="headerlink" title="等式约束条件的优化问题"></a>等式约束条件的优化问题</h1><h2 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h2><p>设 $f(x,y)=x^2+y^2$, 约束条件为： $g(x,y)=xy−1=0$。<br>将三维的 $ f(x,y)$ 图像投影到二维平面上，为下图中红色曲线，也称为 $f(x,y)$ 的等高线。g(x,y) 为图中蓝色曲线。</p><p><img src="/img/lagrange_multiplier.jpg" alt="IMAGE"></p><p>沿着蓝线往内部的圆走，经过橙色点，此时不是最优解，当走到黑色点时，找到了最优解。此时可认为找到了在蓝线这个限制条件下 $f(x,y )$ 的最低点。</p><p>拉格朗日观察到，黑点位置，蓝线与圆是相切的，而橙点位置显然不满足这个性质。且拉格朗日还指出黑点位置蓝线与圆一定是相切的，这正是拉格朗日乘子法的核心。</p><p>在最低点，蓝线的切线方向都指向橙线的等高线方向。换句话说，在切点的位置沿蓝线移动很小的一步，都相当于在橙线的等高线上移动，这个时候，可以认为函数值已经趋于稳定了。所以，我们认为这个点的值“可能”是最低（高）的。<br>相切，意味着在切点的位置，等高线和蓝色曲线的等高线方向是平行的，考虑到梯度与等高线垂直，我们可以用两条曲线的梯度平行来求出切点位置（最低点）。</p><p>所以有：∇f=λ∇g，其中 λ 表示一个标量，因为我们虽然能保证两个梯度平行，但不能保证它们的长度一样（或者方向相同）。在高维函数中，∇f 表示的是函数在各个自变量方向的偏导。对于上面的例子，我们可以求出函数 f 和 g 的偏导，再根据方程组(1)：<br>$\frac{∂f}{∂x}=λ\frac{∂g}{∂x}$<br>$\frac{∂f}{∂y}=λ\frac{∂g}{∂y}$<br>$g(x,y)=0$</p><p>求解时，使用一个统一的拉格朗日函数：$L(x,y,λ)=f(x,y)+λg(x,y)$，令这个函数偏导为 0，可以得到方程组(2)：<br>$\frac{∂L}{∂x}=\frac{∂f}{∂x}−λ\frac{∂g}{∂x}=0$<br>$\frac{∂L}{∂y}=\frac{∂f}{∂y}−λ\frac{∂g}{∂y}=0$<br>$\frac{∂L}{∂λ}=g(x,y)=0$</p><p>可以发现方程组(2)与方程组(1)一样，联立以上三式即可求出 x, y, λ 的值。</p><p>如果是多个约束条件，则拉格朗日函数为：<br>$L(x_1,…,x_n,λ_1,…,λ_k)=f(x1,…,xn)−\sum_{j=1}^{k}λ_jg_j(x_1,…,x_n)$</p><h2 id="进一步理解"><a href="#进一步理解" class="headerlink" title="进一步理解"></a>进一步理解</h2><p>二维情况下的另一个图示，便于理解两个曲线的梯度变化情况：<br><img src="/img/lagrange_multiplier_2.jpg" alt="IMAGE"></p><p>根据拉格朗日乘子法的定义，这是一种寻找极值的策略，换句话说，该方法并不能保证找到的一定是最低点或者最高点。事实上，它只是一种寻找极值点的过程，而且，拉格朗日乘子法找到的切点可能不只一个（也就是上面的方程组可能找到多个解），例如下图：<br><img src="/img/lagrange_multiplier_3.jpg" alt="IMAGE"></p><p>所以联立方程组得到的解为所有极值点，是最优解的必要条件，具体是否为极值点需根据问题本身的具体情况检验. 这个方程组称为等式约束的极值必要条件。如果是凸函数，可以保证最优解是存在的。</p><hr><p><strong>已经解决的在等式约束条件下的求函数极值的问题，那不等式约束条件下，应该如何解决呢？</strong></p><h1 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h1><p>在开始不等式约束条件下求解函数极值之前，先了解凸优化和对偶问题。</p><h2 id="凸优化的意义"><a href="#凸优化的意义" class="headerlink" title="凸优化的意义"></a>凸优化的意义</h2><ol><li>其应用非常广泛，机器学习中很多优化问题都要通过凸优化来求解；</li><li>在非凸优化中，凸优化同样起到重要的作用，很多非凸优化问题，可以转化为凸优化问题来解决；</li><li>如上引用所述，凸优化问题可以看作是具有成熟求解方法的问题，而其他优化问题则未必。</li></ol><p>而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。</p><h2 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h2><p>凸集：定义目标函数和约束函数的定义域。<br>凸函数：定义优化相关函数的凸性限制。<br>凸优化：中心内容的标准描述。<br>凸优化问题求解：核心内容。相关算法，梯度下降法、牛顿法、内点法等。</p><h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>对偶问题是优化问题中非常重要的方法，将一般优化问题转化为凸优化问题，是求解非凸优化问题的有效方法。<br><img src="/img/lagrange_dual.jpg" alt="IMAGE"></p><p>对于一般的优化问题，不管其是不是凸的，其对偶问题一定是凸优化问题。</p><h1 id="不等式约束条件的优化问题"><a href="#不等式约束条件的优化问题" class="headerlink" title="不等式约束条件的优化问题"></a>不等式约束条件的优化问题</h1><h2 id="广义拉格朗日函数"><a href="#广义拉格朗日函数" class="headerlink" title="广义拉格朗日函数"></a>广义拉格朗日函数</h2><p>有不等式约束条件的最优化问题描述如下:<br>$min.:f(x)$<br>$s.t.:$<br>    $g_i(x) ≤ 0, i=1,2,…,p,$<br>    $h_j(x) = 0, j=1,2,…,q,$<br>    $x∈Ω⊂Rn$<br>其中. $f(x)$为目标函数, $g_i(x)≤0,i=1,2,…,p$ 为不等式约束条件, $h_j(x)=0,k=1,2,…,q$为等式约束条件。</p><p>引入广义拉格朗日函数：</p><script type="math/tex; mode=display">L(x,λ,μ)=f(x)+\sum_{i=1}^{p}μ_ig_i(x)+\sum_{j=1}^{q}λ_jh_j(x)</script><p>其中，$μ_i\ge0$。</p><h2 id="定义原始问题最优解"><a href="#定义原始问题最优解" class="headerlink" title="定义原始问题最优解"></a>定义原始问题最优解</h2><p>现在，如果把 $L(x,λ,μ)$ 看作是关于 $λ,μ$ 的函数，经过优化(不管用什么方法)，就是确定的值使得 $L(x,λ,μ)$ 取得最大值(此过程中把 $x$ 看做常量)，确定了 $λ,μ$ 的值，就可以得到 $L(x,λ,μ)$ 的最大值，因为 $λ,μ$ 已经确定，显然 $L(x,λ,μ)$ 的最大值就是只和 $x$ 有关的函数，定义这个函数为：</p><script type="math/tex; mode=display">\theta_p(x)={max}_{λ,μ:μ_i\ge0}L(x,λ,μ)=L(x,λ,μ)</script><p>那么定义 $\theta_p(x)$ 的意义在哪里呢？在于其值就是 f(x)，也就是说 $\theta_p(x)=f(x)$。下面证明这个等式。</p><ol><li>如果 $x$ 违反了约束条件，即 $g_i(x) &gt; 0$ 或者 $h_j(x) \neq 0$，则容易存在 $μ_i -&gt; +∞$，也容易存在 $λ_j$ 使得 $λ_jh_j(x) -&gt; +∞$，这样显然 $\theta_p(x)$ 是没有最大值的。</li><li>如果 $x$ 满足了约束条件，则 $\theta_p(x)={max}_{λ,μ:μ_i\ge0}L(x,λ,μ)={max}_{λ,μ:μ_i\ge0}f(x)=f(x)$，因为 ${max}_{λ,μ:μ_i\ge0}f(x)$ 与 $λ,μ$ 没有关系，所以其等于 $f(x)$。</li></ol><p>所以当 $x$ 满足约束条件时，$min_x\theta_p(x)=min_xf(x)$，我们定义 $p^*=min_x\theta_p(x)$ 代表原始问题的最优解。</p><h2 id="定义对偶问题"><a href="#定义对偶问题" class="headerlink" title="定义对偶问题"></a>定义对偶问题</h2><p>定义关于 $λ,μ$ 的函数：</p><script type="math/tex; mode=display">\theta_D(λ,μ)={min}_xL(x,λ,μ)</script><p>其中 ${min}_xL(x,λ,μ)$ 是关于 $x$ 的函数的最小化，确定 $x$ 以后，最小值就只与 $λ,μ$ 有关，所以是一个关于 $λ,μ$ 的函数。</p><p>考虑极大化 $\theta_D(λ,μ)$ 即 <script type="math/tex">{max}_{λ,μ:μ_i\ge0}\theta_D(λ,μ)={max}_{λ,μ:μ_i\ge0}{min}_xL(x,λ,μ)</script>，与原始问题最优解的定义很相似，形式上是对称的。定义对偶问题的最优解 $d^*={max}_{λ,μ:μ_i\ge0}\theta_D(λ,μ)$</p><h2 id="对偶问题与原始问题的关系"><a href="#对偶问题与原始问题的关系" class="headerlink" title="对偶问题与原始问题的关系"></a>对偶问题与原始问题的关系</h2><p>很显然有 $d^<em> \le p^</em>$，当两个问题的最优解相等时，即 $x^<em>, λ^</em>, μ^<em>$ $d^</em> = p^*$，称原问题和对偶问题是强对偶的，否则是弱对偶的。</p><p>所以为了通过求对偶问题来求原问题，我们希望他们是强对偶的。</p><p>当满足以下条件时他们是强对偶的：</p><ol><li>原始问题是凸优化问题的时候</li><li>原始问题满足 Slater 条件（这一条还可以换成其他条件）</li><li>最优解满足 KKT 条件 </li></ol><h3 id="Slater-条件"><a href="#Slater-条件" class="headerlink" title="Slater 条件"></a>Slater 条件</h3><blockquote><p>若原始问题为凸优化问题，且存在严格满足约束条件的点 x，这里的“严格”是指 $g_i(x)≤0$ 中的“≤”严格取到“&lt;”，即存在 x 满足 $g_i(x)&lt;0 ,i=1,2,…,n$，则存在 $x^∗,α^∗,β^∗$ 使得 $x^∗$ 是原始问题的解， $α^∗,β^∗$ 是对偶问题的解，且满足：<br>$p^∗=d^∗=L(x^∗,α^∗,β^∗)$</p></blockquote><p>也就是说如果原始问题是凸优化问题并且满足 Slater 条件的话，那么强对偶性成立。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。例如，对于某些非凸优化的问题，强对偶也成立。SVM 中的原始问题 是一个凸优化问题（二次规划也属于凸优化问题），Slater 条件在 SVM 中指的是存在一个超平面可将数据分隔开，即数据是线性可分的。当数据不可分时，强对偶是不成立的，这个时候寻找分隔平面这个问题本身也就是没有意义了，所以对于不可分的情况预先加个 kernel 就可以了。</p><h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2><p>KKT 条件(Karush-Kuhn-Tucker Conditions)，是指在满足一些有规则的条件下, 一个非线性规划(Nonlinear Programming)问题能有最优化解法的一个必要和充分条件，这是一个广义化拉格朗日乘数的成果。</p><p>可以发现最优解要么在 $g_i(x) = 0$ 上，要么在 $g_i(x) &lt; 0$ 范围内，如下图：</p><p><img src="/img/lagrange_KKT.jpg" alt="IMAGE"></p><p>定义不等式约束下的拉格朗日函数L：</p><script type="math/tex; mode=display">L(x,λ,μ)=f(x)+\sum_{i=1}^{p}μ_ig_i(x)+\sum_{j=1}^{q}λ_jh_j(x)</script><p>所谓 KKT 最优化条件，就是指上式的最优解$x^*$必须满足下面的条件:</p><ol><li>约束条件满足 $g_i(x^<em>)≤0,i=1,2,…,p$, 以及 $h_j(x^</em>)=0,j=1,2,…,q$</li><li>$∇f(x^<em>)+\sum_{i=1}^{p}μ_i∇g_i(x^</em>)+\sum_{j=1}^{q}λ_j∇h_j(x^*)=0$, 其中$∇$为梯度算子，也就是 L(x,u,λ) 对 x 求导为 0</li><li>$λ_j≠0$ 且不等式约束条件满足 $μ_i≥0$, $μ_ig_i(x^*)=0,i=1,2,…,p$，因为 $g(x)&lt;=0$，如果要满足这个等式，必须 $μ_i=0$ 或者 $g(x)=0$</li></ol><p>第三个式子非常有趣，因为g(x)&lt;=0，如果要满足这个等式，必须$μ_i=0$或者g(x)=0，这是 SVM 的很多重要性质的来源，如支持向量的概念。</p><h3 id="KKT条件的推导"><a href="#KKT条件的推导" class="headerlink" title="KKT条件的推导"></a>KKT条件的推导</h3><p><img src="/img/lagrange_prove_1.jpg" alt="IMAGE"></p><p><img src="/img/lagrange_prove_2.jpg" alt="IMAGE"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于不等式约束条件的优化问题，原问题分两种情况：</p><ul><li>原问题是一个凸优化问题，则可以直接应用 KKT 条件来求解</li><li>原问题不是一个凸优化问题，则通过对偶问题转化为凸优化问题，在满足强对偶的条件下应用 KKT 条件求解</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p><a href="http://jermmy.xyz/2017/07/27/2017-7-27-understand-lagrange-multiplier/" target="_blank" rel="noopener">http://jermmy.xyz/2017/07/27/2017-7-27-understand-lagrange-multiplier/</a><br><a href="https://www.zhihu.com/question/38586401/answer/134473412" target="_blank" rel="noopener">https://www.zhihu.com/question/38586401/answer/134473412</a><br><a href="https://zhuanlan.zhihu.com/p/27731819" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27731819</a><br><a href="http://blog.csdn.net/Mr_KkTian/article/details/53750424" target="_blank" rel="noopener">http://blog.csdn.net/Mr_KkTian/article/details/53750424</a><br><a href="http://www.cnblogs.com/mo-wang/p/4775548.html" target="_blank" rel="noopener">http://www.cnblogs.com/mo-wang/p/4775548.html</a><br><a href="http://www.hanlongfei.com/convex/2015/11/05/duality/" target="_blank" rel="noopener">http://www.hanlongfei.com/convex/2015/11/05/duality/</a><br><a href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html" target="_blank" rel="noopener">https://www.cnblogs.com/90zeng/p/Lagrange_duality.html</a><br><a href="http://bioinfo.ict.ac.cn/~dbu/AlgorithmCourses/Lectures/KKT-examples.pdf" target="_blank" rel="noopener">http://bioinfo.ict.ac.cn/~dbu/AlgorithmCourses/Lectures/KKT-examples.pdf</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;拉格朗日乘子法&quot;&gt;&lt;a href=&quot;#拉格朗日乘子法&quot; class=&quot;headerlink&quot; title=&quot;拉格朗日乘子法&quot;&gt;&lt;/a&gt;拉格朗日乘子法&lt;/h1&gt;&lt;h2 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定
      
    
    </summary>
    
    
      <category term="Math" scheme="http://luojinping.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>导数与微分</title>
    <link href="http://luojinping.com/2018/03/04/%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"/>
    <id>http://luojinping.com/2018/03/04/导数与微分/</id>
    <published>2018-03-04T12:16:31.000Z</published>
    <updated>2019-08-25T13:10:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="连续"><a href="#连续" class="headerlink" title="连续"></a>连续</h1><p>设 $y=f(x)$ 在 $x_0$ 有定义，若<br>$\displaystyle\lim_{x\to x_0}{f(x)}=f(x_0)$<br>则函数 $f(x)$ 在点 $x_0$ 处连续。</p><p>另外一种定义：<br>在点 $x_0$ 附近，如果自变量的该变量是无穷小时，对应的因变量的该变量也是无穷小，则这个函数在点 $x_0$ 处连续。</p><p>通俗地说，所谓“连续”，就是不间断。放到函数上，就是没有“断点”（但是可以有“拐点”）。</p><p>二元函数的连续性定义与一元函数类似。</p><h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>导数（Derivative）是微积分学中重要的基础概念。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。</p><p>导数的几何定义是：曲线上的纵坐标对点的横坐标的导数是曲线在该点的切线的斜率。</p><p>导数的物理意义之一是：位移s对时间t的导数是瞬时速度，即 $v=\frac{ds}{dt}$</p><h2 id="可导"><a href="#可导" class="headerlink" title="可导"></a>可导</h2><p>函数可导定义：<br>（1）设 f(x)在 $x_0$ 及其附近有定义,则当  $\displaystyle\lim_{\Delta{x}\to0}{\frac{f(x_0+\Delta{x})-f(x_0)}{a}}$ 存在, 则称 f(x) 在 $x_0$ 处可导。<br>（2）若对于区间 (a,b) 上任意一点 (x，f(x)) 均可导，则称 f(x) 在 (a，b) 上可导。</p><p>左导数：$\displaystyle\lim_{\Delta{x}\to0^-}{\frac{f(x_0+\Delta{x})-f(x_0)}{a}}$<br>右导数：$\displaystyle\lim_{\Delta{x}\to0^+}{\frac{f(x_0+\Delta{x})-f(x_0)}{a}}$</p><p>连续函数可导条件：函数在该点的左右偏导数都存在且相等。<br>即就是一个函数在某一点求极限，如果极限存在，则为可导，若所得导数等于函数在该点的函数值，则函数为连续可导函数，否则为不连续可导函数。</p><h1 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h1><p>在数学中，微分是对函数的局部变化率的一种线性描述。微分可以近似地描述当函数自变量的取值作足够小的改变时，函数的值是怎样改变的。</p><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>若 y=f(x) 在处可导，则函数的增量，<br>$\Delta{y}=f(x+\Delta{x})-f(x)=f’(x)\Delta{x}+\alpha(\Delta{x})\Delta{x}$<br>其中 $\alpha(\Delta{x})$ 是$\Delta{x}(\Delta{x} \to 0)$ 的高阶无穷小。<br>称 $f’(x)\Delta{x}$ 为 $f(x)$ 在 $x$ 处的微分。<br>而 $f’(x)$ 是 $f(x)$ 在 $x$ 处的导数。</p><h2 id="可微"><a href="#可微" class="headerlink" title="可微"></a>可微</h2><p>设函数$y= f(x)$，若自变量在点x的改变量Δx与函数相应的改变量Δy有关系$Δy=AΔx+ο(Δx)$<br>其中A与Δx无关，则称函数f(x)在点x可微，并称AΔx为函数f(x)在点x的微分，记作dy，即$dy=AΔx$<br>当$x=x_0$时，则记作$dy∣x=x_0$.</p><p>可微条件：<br>必要条件：若函数在某点可微，则该函数在该点对x和y的偏导数必存在。<br>充分条件：若函数对x和y的偏导数在这点的某一邻域内都存在，且均在这点连续，则该函数在这点可微。</p><p>连续但不可微的例子：<br><img src="/img/function_ differential_counterexample.jpg" alt><br>魏尔斯特拉斯函数连续，但在任一点都不可微</p><h1 id="连续，可导，可微的关系"><a href="#连续，可导，可微的关系" class="headerlink" title="连续，可导，可微的关系"></a>连续，可导，可微的关系</h1><ul><li>一元函数：可导必然连续，连续推不出可导，可导与可微等价。</li><li>多元函数：可偏导与连续之间没有联系，也就是说可偏导推不出连续，连续推不出可偏导。</li><li>多元函数中可微必可偏导，可微必连续，可偏导推不出可微，但若一阶偏导具有连续性则可推出可微。</li></ul><h1 id="全微分"><a href="#全微分" class="headerlink" title="全微分"></a>全微分</h1><p>对于多元函数，如果两个自变量都变化的话，这时候函数的微分就称为全微分。如果函数z=f(x,y)在(x,y)处的全增量$\Delta z=f(x+\Delta x,y+\Delta y)-f(x,y)$可以写成$\Delta z=A\Delta x+B\Delta y+o(\rho)$，<br>取其线性主部，称为二元函数的全微分 $\Delta z=A\Delta x+B\Delta y$</p><p>可以证明全微分又可写成$dz=f_x dx+f_ydy =\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}dy$</p><p>上式又称为全微分的叠加原理。可以这么理解：全增量包含两个部分，$\Delta x$引起的函数值增量和$\Delta y$引起的函数值增量。</p><p>一句话感性说全微分其实就在表达曲面在某一点处的切平面。</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。</p><p>例如：在一元函数 f(x) 中，梯度只能沿 x 轴正方向或负方向，而在二元函数 f(x,y) 中，梯度则是一个二维向量 (∂f/∂x,∂f/∂y)。</p><p>梯度一个重要的性质：梯度跟函数等高线是垂直的。<br>证明：<br>假设 Δx，Δy 是两个极小的变化量，根据全微分的知识，可以得到：<br>f(x+Δx,y+Δy)≈f(x,y)+∂f∂xΔx+∂f∂yΔy<br>如果 (Δx,Δy) 是在等高线方向的增量，那么 f(x+Δx,y+Δy)≈f(x,y)，这意味着 ∂f∂xΔx+∂f∂yΔy=0，换句话说，向量 ∇f 和向量 (Δx,Δy) 的内积为 0。所以，梯度和函数的等高线是垂直的。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://jermmy.xyz/2017/07/27/2017-7-27-understand-lagrange-multiplier/" target="_blank" rel="noopener">http://jermmy.xyz/2017/07/27/2017-7-27-understand-lagrange-multiplier/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;连续&quot;&gt;&lt;a href=&quot;#连续&quot; class=&quot;headerlink&quot; title=&quot;连续&quot;&gt;&lt;/a&gt;连续&lt;/h1&gt;&lt;p&gt;设 $y=f(x)$ 在 $x_0$ 有定义，若&lt;br&gt;$\displaystyle\lim_{x\to x_0}{f(x)}=f(x_0)
      
    
    </summary>
    
    
      <category term="Math" scheme="http://luojinping.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>概率论知识</title>
    <link href="http://luojinping.com/2018/02/25/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%9F%A5%E8%AF%86/"/>
    <id>http://luojinping.com/2018/02/25/概率论知识/</id>
    <published>2018-02-25T02:34:09.000Z</published>
    <updated>2019-08-25T13:10:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。</p><p>随机变量在概率论中扮演着一个重要角色。最重要的一个事实是，随机变量并不是变量，它们实际上是将（样本空间中的）结果映射到真值的函数。我们通常用一个大写字母来表示随机变量。 </p><h1 id="条件分布"><a href="#条件分布" class="headerlink" title="条件分布"></a>条件分布</h1><p>条件分布为概率论中用于探讨不确定性的关键工具之一。它明确了在另一随机变量已知的情况下（或者更通俗来说，当已知某事件为真时）的某一随机变量的分布。 </p><p>正式地，给定$Y=b$时，$X=a$的条件概率定义为： </p><script type="math/tex; mode=display">P(X=a|Y=b)= \frac{P(X=a,Y=b)}{P(Y=b)}</script><p>其中，$P(Y=b)&gt;0$</p><h1 id="独立性"><a href="#独立性" class="headerlink" title="独立性"></a>独立性</h1><p>在概率论中，独立性是指随机变量的分布不因知道其它随机变量的值而改变。在机器学习中，我们通常都会对数据做这样的假设。例如，我们会假设训练样本是从某一底层空间独立提取；并且假设样例i的标签独立于样例j(i≠j)的特性。<br>从数学角度来说，随机变量X独立于Y，当： </p><p>P(X)=P(X|Y)</p><p>注意，上式没有标明X,Y的取值，也就是说该公式对任意X,Y可能的取值均成立。）<br>利用等式(2)，很容易可以证明如果X对Y独立，那么Y也独立于X。当X和Y相互独立时，记为X⊥Y。<br>对于随机变量X和Y的独立性，有一个等价的数学公式：<br>P(X,Y)=P(X)P(Y)</p><p>我们有时也会讨论条件独立，就是当我们当我们知道一个随机变量（或者更一般地，一组随机变量）的值时，那么其它随机变量之间相互独立。正式地，我们说“给定Z，X和Y条件独立”，如果：<br>P(X|Z)=P(X|Y,Z)</p><p>或者等价的：<br>P(X,Y|Z)=P(X|Z)P(Y|Z)</p><h1 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h1><p>我们现在给出两个与联合分布和条件分布相关的，基础但是重要的可操作定理。第一个叫做链式法则，它可以看做等式(2)对于多变量的一般形式。<br>定理1（链式法则）： </p><p>P(X1,X2,…,Xn)=P(X1)P(X2|X1)…P(Xn|X1,X2,…,Xn−1)…………(3)</p><p>链式法则通常用于计算多个随机变量的联合概率，特别是在变量之间相互为（条件）独立时会非常有用。注意，在使用链式法则时，我们可以选择展开随机变量的顺序；选择正确的顺序通常可以让概率的计算变得更加简单。<br>第二个要介绍的是贝叶斯定理。利用贝叶斯定理，我们可以通过条件概率P(Y|X)计算出P(X|Y)，从某种意义上说，就是“交换”条件。它也可以通过等式(2)推导出。</p><h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><h2 id="条件概率-1"><a href="#条件概率-1" class="headerlink" title="条件概率"></a>条件概率</h2><p>如果 A，B 是条件组 S 下的随机事件，事件 A 发生的概率随事件 B 是否发生而变化，同样，事件 B 发生的概率也随事件 A 是否发生而变化。<br>事件 A 在另外一个事件 B 已经发生条件下的发生概率称为条件概率，表示为P(A|B)，读作「在 B 条件下 A 的概率」。<br>当 P(B) &gt; 0 时，有：</p><script type="math/tex; mode=display">P(A|B)= \frac{P(AB)}{P(B)}</script><p>P.S. 如果 A，B 是独立事件，则 A 发生的概率与 B 无关，那么 $P(A|B) = P(A)$，并且 $P(AB)=P(A)P(B)$。</p><h2 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h2><p>联合概率表示两个事件共同发生的概率。A 与 B 的联合概率表示为<br>$P(A\cap B)$ 或者 ${\displaystyle P(A,B)}$ 或者 $P(A,B)$。</p><h2 id="边缘概率"><a href="#边缘概率" class="headerlink" title="边缘概率"></a>边缘概率</h2><p>边缘概率是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中不需要的那些事件合并成其事件的全概率而消失（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率）。这称为边缘化（marginalization）。A的边缘概率表示为$P(A)$，B的边缘概率表示为$P(B)$。</p><h1 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h1><p><img src="/img/law_of_total_probability.jpg" alt="law of total probability"> </p><h1 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h1><script type="math/tex; mode=display">P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}</script><p><a href="https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87</a><br><a href="http://blog.csdn.net/u012566895/article/details/51220127" target="_blank" rel="noopener">http://blog.csdn.net/u012566895/article/details/51220127</a><br><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="noopener">http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。&lt;/p&gt;
&lt;p&gt;随机变量在概率论中扮演着一个重要角色。最重要的一个事实是，随机变量并不是变量，它们实际上是将（样本空间中的）结果映射到真值的函数。我们通常用一个大写字母来表示随机变量。 
      
    
    </summary>
    
    
      <category term="Math" scheme="http://luojinping.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 6 生成学习算法</title>
    <link href="http://luojinping.com/2018/02/25/Stanford-Machine-Learning-6-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    <id>http://luojinping.com/2018/02/25/Stanford-Machine-Learning-6-生成学习算法/</id>
    <published>2018-02-25T02:24:02.000Z</published>
    <updated>2019-08-25T13:10:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生成学习算法介绍"><a href="#生成学习算法介绍" class="headerlink" title="生成学习算法介绍"></a>生成学习算法介绍</h1><p>有监督机器学习可以分为判别学习算法(generative learning algorithm)和生成学习算法(discriminative learning algorithm)。</p><ul><li>判别学习算法常见的有：逻辑回顾，支持向量机等。</li><li>生成学习算法常见的有：混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等。</li></ul><p>判别学习算法是直接学习 p(y|x) 或者是从输入直接映射到输出的算法。</p><p>生成学习算法是计算变量x在变量y上的条件分布p(x|y)和变量y的分布p(y) ，然后使用贝叶斯公式:  $p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(y)*p(x|y)}{p(x)}$ 计算出p(y|x)。</p><p>针对课程中提到的两种生成学习算法中，高斯判别分析(Gaussian Discriminant Analysis)和朴素贝叶斯(Navie Bayes)分别解决了两种场景下的问题。<br>GDA 是针对的是特征向量 X 为连续值时的问题，而 Navie Bayes 则针对的是特征向量为离散值时的问题。</p><h1 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h1><h2 id="多维正态分布-The-multivariate-normal-distribution"><a href="#多维正态分布-The-multivariate-normal-distribution" class="headerlink" title="多维正态分布(The multivariate normal distribution)"></a>多维正态分布(The multivariate normal distribution)</h2><p>假设随机变量 $X$ 满足 $n$ 维的多项正态分布，参数为均值向量 $μ ∈ R^{n} $，协方差矩阵$Σ ∈ R^{n×n}$，记为 $N(μ,Σ)$ 其概率密度表示为：</p><script type="math/tex; mode=display">p(x;μ,Σ)=\frac{1}{(2π)^{\frac{n}2}(detΣ)^{\frac12}}exp(−\frac12(x−μ)^TΣ^{−1}(x−μ))</script><p>$detΣ$ 表示矩阵 $Σ$ 的行列式(determinant)。<br>均值向量: $μ$<br>协方差矩阵: $Σ=E[(X−E[X])(X−E[X])T]=E[(x−μ)(x−μ)T]$</p><h2 id="高斯判别分析-1"><a href="#高斯判别分析-1" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h2><p>GDA 模型针对的是输入特征为连续值时的分类问题，这个模型的基本假设是目标值 y 服从伯努利分布(0-1分布)，条件概率 P(x|y) 服从多元正态分布((multivariate normal distribution))，即:<br>$y∼Bernoulli(\phi)$<br>$P(x|y=0)∼N(μ_0,\Sigma)$<br>$P(x|y=1)∼N(μ_1,\Sigma)$</p><p>它们的概率密度为：</p><script type="math/tex; mode=display">p(y)=\phi^y(1−\phi)^{1−y}</script><script type="math/tex; mode=display">p(x|y=0)=\frac1{(2π)^{n/2}|\Sigma|^{1/2}}exp(−\frac12(x−μ_0)^T\Sigma^{−1}(x−μ_0))</script><script type="math/tex; mode=display">p(x|y=1)=\frac1{(2π)^{n/2}|\Sigma|^{1/2}}exp(−\frac12(x−μ_1)^T\Sigma^{−1}(x−μ_1))</script><p>我们模型的参数包括，$\phi,\Sigma,μ_0,μ_1$ 注意到，我们使用了两种不同的均值向量$μ_0$和$μ_1$，但是使用了同一种协方差矩阵 $\Sigma$, 则我们的极大似然函数的对数如下所示：</p><script type="math/tex; mode=display">L(\phi,μ_0,μ_1,\Sigma)=log\Pi_{i=1}^mp(x^{(i)},y^{(i)};\phi,μ_0,μ_1,\Sigma)</script><script type="math/tex; mode=display">=log\Pi_{i=1}^mp(x^{(i)}|y^{(i)};\phi,μ_0,μ_1,\Sigma)p(y^{(i)};\phi)</script><p>对极大似然函数对数最大化，我们就得到了GDA模型各参数的极大虽然估计(略)。</p><h1 id="GDA-与-LR"><a href="#GDA-与-LR" class="headerlink" title="GDA 与 LR"></a>GDA 与 LR</h1><p>前面我们提到：</p><script type="math/tex; mode=display">{argmax}_yp(y|x)={argmax}_y\frac{p(x|y)p(y)}{p(x)}={argmax}_yp(x|y)p(y)</script><p>我们有：</p><script type="math/tex; mode=display">p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}</script><p>上式实际上可以表示成logistic函数的形式：</p><script type="math/tex; mode=display">p(y=1|x;ϕ,μ0,μ1,Σ)=\frac1{1+exp(−θ^TX)}</script><p>其中，θ是参数ϕ,μ0,μ1,Σθ是参数ϕ,μ0,μ1,Σ某种形式的函数。GDA的后验分布可以表示logistic函数的形式。<br>　<br>下图为用 GDA 对两类样本分别拟合高斯概率密度函数p(x|y=0)和p(x|y=1)，得到两个钟形曲线。沿x轴遍历样本，在x轴上方画出相应的p(y=1|x)。如选x轴靠左的点，那么它属于1的概率几乎为0，p(y=1|x)=0，两条钟形曲线交点处，属于0或1的概率相同，p(y=1|x)=0.5，x轴靠右的点，输出1的概率几乎为1，p(y=1|x)=1。最终发现，得到的曲线和sigmoid函数曲线很相似。<br><img src="/img/gaussian_discriminant_analysis_probability_distribution.jpg" alt="gaussian discriminant analysis probability distribution"><br>　<br>　<br>实际上，可以证明，不仅仅当先验概率分布服从多变量正态分布时可以推导出逻辑回归的模型，当先验分布属于指数分布簇中的任何一个分布，如泊松分布时，都可以推导出逻辑回归模型。而反之不成立，逻辑回归的先验概率分布不一定必须得是指数分布簇中的成员。基于这些原因，在实践中使用逻辑回归比使用GDA更普遍。</p><p>生成学习算法比判决学习算法需要更少的数据。如GDA的假设较强，所以用较少的数据能拟合出不错的模型。而逻辑回归的假设较弱，对模型的假设更为健壮，拟合数据需要更多的样本。</p><h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><p>考虑自变量比较多的情况，比如垃圾邮件的识别需要检测成百上千甚至上万的字符是否出现，如有<em>免费</em>、<em>购买</em>等类似的词出现的邮件很大可能是垃圾邮件。这种情况下若有k个自变量，考虑各变量之间的交互作用就需要计算$2^k$次，为了简化计算量对模型作一个更强的假设：<br><strong>给定因变量 y 的值，各自变量之间相互独立.</strong></p><p>所以有</p><script type="math/tex; mode=display">p(x_1,...,x_n|y)=p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)...p(x_n|y,x_1,x_2,...,x_{n-1})</script><script type="math/tex; mode=display">=p(x_1|y)p(x_2|y)p(x_3|y)…p(x_n|y)=\Pi_{i=1}^np(x_i|y)</script><p>第一个等式是根据通常的概率论得到的，第二个等式是根据贝叶斯假设得到的。虽然贝叶斯假设是个很强的假设，但是实践证明在许多问题上都表现得很好。</p><p>参数的极大似然估计及p(y|x)的推导过程略。</p><h1 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h1><p>拉普拉斯平滑(Laplace Smoothing)又称为加1平滑。平滑方法的存在是为了解决零概率问题。</p><p>所谓的零概率问题，就是在计算新实例的概率时，如果某个分量在训练集中从没出现过，会导致整个实例的概率计算结果为０，针对文本分类问题就是当一个词语在训练集中没有出现过，那么该词语的概率为０，使用连乘计算文本出现的概率时，整个文本出现的概率也为０，这显然不合理，因为不能因为一个事件没有观测到就判断该事件的概率为０.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://xtf615.com/2017/03/25/%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">http://xtf615.com/2017/03/25/%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/</a><br><a href="http://blog.csdn.net/v1_vivian/article/details/52190572" target="_blank" rel="noopener">http://blog.csdn.net/v1_vivian/article/details/52190572</a><br><a href="http://www.cnblogs.com/mikewolf2002/p/7763475.html" target="_blank" rel="noopener">http://www.cnblogs.com/mikewolf2002/p/7763475.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;生成学习算法介绍&quot;&gt;&lt;a href=&quot;#生成学习算法介绍&quot; class=&quot;headerlink&quot; title=&quot;生成学习算法介绍&quot;&gt;&lt;/a&gt;生成学习算法介绍&lt;/h1&gt;&lt;p&gt;有监督机器学习可以分为判别学习算法(generative learning algorith
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Binary-Tree-Maximum-Path-Sum</title>
    <link href="http://luojinping.com/2018/01/31/LeetCode-Binary-Tree-Maximum-Path-Sum/"/>
    <id>http://luojinping.com/2018/01/31/LeetCode-Binary-Tree-Maximum-Path-Sum/</id>
    <published>2018-01-31T00:13:03.000Z</published>
    <updated>2019-08-25T02:50:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Given a binary tree, find the maximum path sum.</p><p>For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root.</p><p>For example:<br>Given the below binary tree,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  1</span><br><span class="line"> / \</span><br><span class="line">2   3</span><br></pre></td></tr></table></figure></p><p>Return 6.</p><p><strong> problem link: </strong><br><a href="https://leetcode.com/problems/binary-tree-maximum-path-sum/description/" target="_blank" rel="noopener">https://leetcode.com/problems/binary-tree-maximum-path-sum/description/</a></p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> maxValue;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxPathSum</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        maxValue = Integer.MIN_VALUE;</span><br><span class="line">        maxPathDown(root);</span><br><span class="line">        <span class="keyword">return</span> maxValue;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">maxPathDown</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> left = Math.max(<span class="number">0</span>, maxPathDown(node.left));</span><br><span class="line">        <span class="keyword">int</span> right = Math.max(<span class="number">0</span>, maxPathDown(node.right));</span><br><span class="line">        maxValue = Math.max(maxValue, left + right + node.val);</span><br><span class="line">        <span class="keyword">return</span> Math.max(left, right) + node.val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>每一个结点可以选和不选，处理方法就是：<code>int left = Math.max(0, maxPathDown(node.left));</code>，其中的 Math.max(0, x)，当取值为 0 时就是不取这个结点。</p><p>全局变量 maxValue 就覆盖了子树中的 ^ 这种类型，例如子树如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">       x</span><br><span class="line">   a     y</span><br><span class="line">b    c</span><br></pre></td></tr></table></figure><p>则 b-&gt;a-&gt;c 这种路径的最大值被 maxValue 保存了。而 b-&gt;a-&gt;x-&gt;y 这种经过根节点的路径被 <code>Math.max(left, right) + node.val;</code> 覆盖了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class=&quot;headerlink&quot; title=&quot;Problem Description&quot;&gt;&lt;/a&gt;Problem Description&lt;/h2&gt;&lt;p&gt;Gi
      
    
    </summary>
    
    
      <category term="Algorithm" scheme="http://luojinping.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Trapping-Rain-Water</title>
    <link href="http://luojinping.com/2018/01/31/LeetCode-Trapping-Rain-Water/"/>
    <id>http://luojinping.com/2018/01/31/LeetCode-Trapping-Rain-Water/</id>
    <published>2018-01-31T00:10:48.000Z</published>
    <updated>2019-08-25T02:50:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining.</p><p>For example,<br>Given [0,1,0,2,1,0,1,3,2,1,2,1], return 6.</p><p><strong> problem link: </strong><br><a href="https://leetcode.com/problems/trapping-rain-water" target="_blank" rel="noopener">https://leetcode.com/problems/trapping-rain-water</a></p><p><img src="/img/LeetCode_Trapping_Rain_Water.jpg" alt></p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TrappingRainWater_42</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">trap</span><span class="params">(<span class="keyword">int</span>[] height)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> b = height.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> leftMax = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> rightMax = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (a &lt;= b) &#123;</span><br><span class="line">            leftMax = Math.max(leftMax, height[a]);</span><br><span class="line">            rightMax = Math.max(rightMax, height[b]);</span><br><span class="line">            <span class="keyword">if</span> (leftMax &lt; rightMax) &#123;</span><br><span class="line">                <span class="comment">// leftMax is smaller than rightMax, so the (leftMax-A[a]) water can be stored</span></span><br><span class="line">                max += (leftMax - height[a]);</span><br><span class="line">                a++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                max += (rightMax - height[b]);</span><br><span class="line">                b--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h2><p>对任意位置 i，在 i 上的积水，由左右两边最高的 bar 决定。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class=&quot;headerlink&quot; title=&quot;Problem Description&quot;&gt;&lt;/a&gt;Problem Description&lt;/h2&gt;&lt;p&gt;Gi
      
    
    </summary>
    
    
      <category term="Algorithm" scheme="http://luojinping.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Consumer 的实现</title>
    <link href="http://luojinping.com/2017/11/12/Kafka-Consumer-%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://luojinping.com/2017/11/12/Kafka-Consumer-的实现/</id>
    <published>2017-11-12T07:16:54.000Z</published>
    <updated>2019-08-25T03:49:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>_说明: kafka 版本号为 0.11.0_</strong></p><h1 id="Consumer-拉取消息的实现"><a href="#Consumer-拉取消息的实现" class="headerlink" title="Consumer 拉取消息的实现"></a>Consumer 拉取消息的实现</h1><p>在 Kafka Consumer 正常消费时，观察其调用堆栈。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">"pool-16-thread-7" #154 prio=5 os_prio=0 tid=0x00007ff581c8c000 nid=0x326d runnable [0x00007ff5468e7000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        ...</span><br><span class="line">        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:433)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:232)</span><br><span class="line">        - locked &lt;0x00000000c2e04f90&gt; (a org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)</span><br><span class="line">        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)</span><br><span class="line">        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)</span><br><span class="line">        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:571)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>对应的代码实现是 <code>org.apache.kafka.clients.consumer.KafkaConsumer#poll</code>，如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout)</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">// poll for new data until the timeout expires</span></span><br><span class="line">            <span class="keyword">long</span> start = time.milliseconds();</span><br><span class="line">            <span class="keyword">long</span> remaining = timeout;</span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">                Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollOnce(remaining);</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests())</span><br><span class="line">                        client.pollNoWakeup();</span><br><span class="line">                        </span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> ConsumerRecords&lt;&gt;(records);</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">                        </span><br><span class="line">                <span class="keyword">long</span> elapsed = time.milliseconds() - start;</span><br><span class="line">                remaining = timeout - elapsed;</span><br><span class="line">            &#125; <span class="keyword">while</span> (remaining &gt; <span class="number">0</span>);</span><br><span class="line">            <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            release();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>其中 <code>org.apache.kafka.clients.consumer.KafkaConsumer#pollOnce</code>的实现如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollOnce(<span class="keyword">long</span> timeout) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// ConsumerCoordinator coordinator;</span></span><br><span class="line">        coordinator.poll(time.milliseconds(), timeout);</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment">// if data is available already, return it immediately</span></span><br><span class="line">        Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">        <span class="keyword">if</span> (!records.isEmpty())</span><br><span class="line">            <span class="keyword">return</span> records;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// send any new fetches (won't resend pending fetches)</span></span><br><span class="line">        fetcher.sendFetches();</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>所以可以看到 consumer 每次 poll 时是先从 fetcher 中 fetchedRecords 的，如果拿不到结果，就新发起一个 sendFetches 请求。</p><h2 id="Consumer-拉取消息的数量"><a href="#Consumer-拉取消息的数量" class="headerlink" title="Consumer 拉取消息的数量"></a>Consumer 拉取消息的数量</h2><p>在 <code>org.apache.kafka.clients.consumer.internals.Fetcher#fetchedRecords</code> 可以看到 <code>maxPollRecords</code>(max.poll.records 配置) 变量限制了每次 poll 的消息条数，不管 consumer 对应多少个 partition，从所有 partition 拉取到的消息条数总和不会超过 <code>maxPollRecords</code>。</p><p>在 <code>org.apache.kafka.clients.consumer.internals.Fetcher#sendFetches</code> 可以看到 <code>fetchSize</code>(max.partition.fetch.bytes 配置) 用于每次创建 FetchRequest 时的 <code>org.apache.kafka.common.requests.FetchRequest.PartitionData</code> 的参数设置。<code>fetchSize</code>限制了 consumer 每次从每个 partition 拉取的数据量。<br>不过，还是看代码中的 <code>ConsumerConfig#MAX_PARTITION_FETCH_BYTES_DOC</code> 说明吧：</p><blockquote><p>The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (topic config). See “ + FETCH_MAX_BYTES_CONFIG + “ for limiting the consumer request size.</p></blockquote><h3 id="poll-和-fetch-的关系"><a href="#poll-和-fetch-的关系" class="headerlink" title="poll 和 fetch 的关系"></a>poll 和 fetch 的关系</h3><p>在满足max.partition.fetch.bytes限制的情况下，假如fetch到了100个record，放到本地缓存后，由于max.poll.records限制每次只能poll出15个record。那么KafkaConsumer就需要执行7次才能将这一次通过网络发起的fetch请求所fetch到的这100个record消费完毕。其中前6次是每次pool中15个record，最后一次是poll出10个record。</p><h1 id="Consumer-的心跳机制"><a href="#Consumer-的心跳机制" class="headerlink" title="Consumer 的心跳机制"></a>Consumer 的心跳机制</h1><p>在 <code>org.apache.kafka.clients.consumer.internals.AbstractCoordinat</code> 中启动 <code>HeartbeatThread</code> 线程来定时发送心跳和检查 consumer 的状态。<br>每个 Consumer 都有一个 ConsumerCoordinator(继承 AbstractCoordinator)，每个 ConsumerCoordinator 都启动一个 <code>HeartbeatThread</code> 线程来维护心跳，心跳信息存放在 <code>org.apache.kafka.clients.consumer.internals.Heartbeat</code>。</p><p>实现如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Heartbeat thread for group &#123;&#125; started"</span>, groupId);</span><br><span class="line">                <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                        ...</span><br><span class="line">                        client.pollNoWakeup();</span><br><span class="line">                        <span class="keyword">long</span> now = time.milliseconds();</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> (coordinatorUnknown()) &#123;</span><br><span class="line">                            ...</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (heartbeat.sessionTimeoutExpired(now)) &#123;</span><br><span class="line">                            <span class="comment">// the session timeout has expired without seeing a successful heartbeat, so we should</span></span><br><span class="line">                            <span class="comment">// probably make sure the coordinator is still healthy.</span></span><br><span class="line">                            coordinatorDead();</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (heartbeat.pollTimeoutExpired(now)) &#123;</span><br><span class="line">                            <span class="comment">// the poll timeout has expired, which means that the foreground thread has stalled</span></span><br><span class="line">                            <span class="comment">// in between calls to poll(), so we explicitly leave the group.</span></span><br><span class="line">                            maybeLeaveGroup();</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!heartbeat.shouldHeartbeat(now)) &#123;</span><br><span class="line">                            <span class="comment">// poll again after waiting for the retry backoff in case the heartbeat failed or the</span></span><br><span class="line">                            <span class="comment">// coordinator disconnected</span></span><br><span class="line">                            AbstractCoordinator.<span class="keyword">this</span>.wait(retryBackoffMs);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            heartbeat.sentHeartbeat(now);</span><br><span class="line">                            ...</span><br><span class="line">                        &#125;</span><br><span class="line">                  &#125; <span class="comment">// end synchronized</span></span><br><span class="line">              &#125; <span class="comment">// end while</span></span><br><span class="line">          &#125; <span class="comment">//end try              </span></span><br><span class="line">&#125; <span class="comment">// end run</span></span><br></pre></td></tr></table></figure></p><p>其中最重要的两个 timeout 函数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">sessionTimeoutExpired</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> now - Math.max(lastSessionReset, lastHeartbeatReceive) &gt; sessionTimeout;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">pollTimeoutExpired</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> now - lastPoll &gt; maxPollInterval;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="sessionTimeout"><a href="#sessionTimeout" class="headerlink" title="sessionTimeout"></a>sessionTimeout</h2><p>如果是 sessionTimeout 则 Mark the current coordinator as dead，此时  会将 consumer 踢掉，重新分配 partition 和 consumer 的对应关系。</p><p>在 Kafka Server 端，Consumer 的 Group 定义了五个状态：：<br><img src="/img/kafka_consumer_state_in_server.png" alt="Consumer Group State"></p><p></p><h2 id="pollTimeout"><a href="#pollTimeout" class="headerlink" title="pollTimeout"></a>pollTimeout</h2><p>如果是 pollTimeout 则 Reset the generation and memberId because we have fallen out of the group，此时 consumer 会退出 group，当再次 poll 时又会 rejoin group 触发 rebalance group。</p><h3 id="Rebalance-Generation"><a href="#Rebalance-Generation" class="headerlink" title="Rebalance Generation"></a>Rebalance Generation</h3><p>表示 rebalance 之后的一届成员，主要是用于保护 consumer group，隔离无效 offset 提交。每次 group 进行 rebalance 之后，generation 号都会加 1，表示 group 进入到了一个新的版本，下图所示为 consumer 2 退出后 consumer 4 加入时 Rebalance Generation 的过程：<br><img src="/img/Kafka_Rebalance_Generation.png" alt="Rebalance Generation"></p><h1 id="partition-的数量设置"><a href="#partition-的数量设置" class="headerlink" title="partition 的数量设置"></a>partition 的数量设置</h1><ul><li><p>一个 partition 只能被 Consumer Group 中的一个 consumer 消费，因此，为了提高并发量，可以提高 partition 的数量，但是这会造成 replica 副本拷贝的网络请求增加，故障恢复时的耗时增加。因为 kafka 使用 batch pull 的方式，所以单个线程的消费速率还是有保障的。并且 partition 数量过多，zk 维护 ISR 列表负载较重。</p></li><li><p>partiton 数量最好是 consumer 数目的整数倍，比如取 24， consumer 数目的设置就会灵活很多。</p></li><li><p>consumer 消费消息时不时严格有序的。当从多个 partition 读数据时，kafka 只保证在一个 partition 上数据是有序的，多个 partition 的消息消费很可能就不是严格有序的了。</p></li></ul><h1 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h1><h2 id="heartbeat-interval-ms"><a href="#heartbeat-interval-ms" class="headerlink" title="heartbeat.interval.ms"></a>heartbeat.interval.ms</h2><p>心跳间隔。心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。<br>这个值必须设置的小于 session.timeout.ms，因为：<br>当 consumer 由于某种原因不能发 heartbeat 到 coordinator 时，并且时间超过 session.timeout.ms 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。</p><h3 id="参数值"><a href="#参数值" class="headerlink" title="参数值"></a>参数值</h3><p>默认值：3000 (3s)，通常设置的值要低于session.timeout.ms的1/3。</p><p></p><p></p><h3 id="session-timeout-ms"><a href="#session-timeout-ms" class="headerlink" title="session.timeout.ms"></a>session.timeout.ms</h3><p>consumer session 过期时间。如果超时时间范围内，没有收到消费者的心跳，broker 会把这个消费者置为失效，并触发消费者负载均衡。因为只有在调用 poll 方法时才会发送心跳，更大的 session 超时时间允许消费者在 poll 循环周期内处理消息内容，尽管这会有花费更长时间检测失效的代价。如果想控制消费者处理消息的时间，</p><h3 id="参数值-1"><a href="#参数值-1" class="headerlink" title="参数值"></a>参数值</h3><p>默认值：10000 (10s)，这个值必须设置在 broker configuration 中的 group.min.session.timeout.ms 与 group.max.session.timeout.ms 之间。</p><p></p><h2 id="max-poll-interval-ms"><a href="#max-poll-interval-ms" class="headerlink" title="max.poll.interval.ms"></a>max.poll.interval.ms</h2><p>This config sets the maximum delay between client calls to poll(). </p><p>When the timeout expires, the consumer will stop sending heartbeats and send an explicit LeaveGroup request. </p><p>As soon as the consumer resumes processing with another call to poll(), the consumer will <strong>rejoin the group</strong>. </p><p>By increasing the interval between expected polls, you can give the consumer more time to handle a batch of records returned frompoll(long). The drawback is that increasing this value may delay a group rebalance since the consumer will only join the rebalance inside the call to poll. You can use this setting to bound the time to finish a rebalance, but you risk slower progress if the consumer cannot actually call poll often enough.</p><p>参数设置大一点可以增加两次 poll 之间处理消息的时间。<br>当 consumer 一切正常(也就是保持着 heartbeat )，且参数的值小于消息处理的时长，会导致 consumer leave group 然后又 rejoin group，触发无谓的 group balance，出现 consumer livelock 现象。</p><p>但如果设置的太大，会延迟 group rebalance，因为消费者只会在调用 poll 时加入rebalance。</p><p></p><h2 id="max-poll-records"><a href="#max-poll-records" class="headerlink" title="max.poll.records"></a>max.poll.records</h2><p>Use this setting to limit the total records returned from a single call to poll. This can make it easier to predict the maximum that must be handled within each poll interval. By tuning this value, you may be able to reduce the poll interval, which will reduce the impact of group rebalancing.</p><p>0.11.0 Kafka 的默认配置是 </p><ul><li>max.poll.interval.ms=5min</li><li>max.poll.records=500</li></ul><p>即平均 600ms 要处理完一条消息，如果消息的消费时间高于 600ms，则一定要调整 max.poll.records 或 max.poll.interval.ms。</p><h1 id="Kafka-Javadoc-Detecting-Consumer-Failures"><a href="#Kafka-Javadoc-Detecting-Consumer-Failures" class="headerlink" title="Kafka Javadoc - Detecting Consumer Failures"></a>Kafka Javadoc - Detecting Consumer Failures</h1><p>After subscribing to a set of topics, the consumer will automatically join the group when poll(long) is invoked. The poll API is designed to ensure consumer liveness. As long as you continue to call poll, the consumer will stay in the group and continue to receive messages from the partitions it was assigned. Underneath the covers, the consumer sends periodic heartbeats to the server. If the consumer crashes or is unable to send heartbeats for a duration of session.timeout.ms, then the consumer will be considered dead and its partitions will be reassigned.<br>It is also possible that the consumer could encounter a “livelock” situation where it is continuing to send heartbeats, but no progress is being made. To prevent the consumer from holding onto its partitions indefinitely in this case, we provide a liveness detection mechanism using the max.poll.interval.ms setting. Basically if you don’t call poll at least as frequently as the configured max interval, then the client will proactively leave the group so that another consumer can take over its partitions. When this happens, you may see an offset commit failure (as indicated by a CommitFailedException thrown from a call to commitSync()). This is a safety mechanism which guarantees that only active members of the group are able to commit offsets. So to stay in the group, you must continue to call poll. </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://www.cnblogs.com/huxi2b/p/6223228.html" target="_blank" rel="noopener">Kafka消费组(consumer group)</a><br><a href="https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html" target="_blank" rel="noopener">kafka.apache.org javadoc</a><br><a href="http://blog.leanote.com/post/zfb050/Coordinator%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86" target="_blank" rel="noopener">Coordinator实现原理</a><br><a href="http://debugo.com/kafka-params/" target="_blank" rel="noopener">kafka params</a><br><a href="http://blog.csdn.net/u014393917/article/details/52043185" target="_blank" rel="noopener">kafka源码分析之kafka的consumer的负载均衡管理</a><br><a href="http://www.cnblogs.com/devos/p/5656232.html" target="_blank" rel="noopener">Group Management Protocol</a><br><a href="http://matt33.com/2017/01/16/kafka-group/" target="_blank" rel="noopener">Kafka 之 Group 状态变化分析及 Rebalance 过程</a><br><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread" target="_blank" rel="noopener">KIP-62: Allow consumer to send heartbeats from a background thread</a><br><a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html" target="_blank" rel="noopener">Kafka: The Definitive Guide Chapter 4 - Kafka Consumers</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;_说明: kafka 版本号为 0.11.0_&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;Consumer-拉取消息的实现&quot;&gt;&lt;a href=&quot;#Consumer-拉取消息的实现&quot; class=&quot;headerlink&quot; title=&quot;Consumer 拉取消
      
    
    </summary>
    
    
      <category term="Distributed System" scheme="http://luojinping.com/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>解决 Kafka Consumer 卡顿的问题</title>
    <link href="http://luojinping.com/2017/11/12/%E8%A7%A3%E5%86%B3-Kafka-Consumer-%E5%8D%A1%E9%A1%BF%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://luojinping.com/2017/11/12/解决-Kafka-Consumer-卡顿的问题/</id>
    <published>2017-11-12T07:15:18.000Z</published>
    <updated>2019-08-25T03:48:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="运行环境说明"><a href="#运行环境说明" class="headerlink" title="运行环境说明"></a>运行环境说明</h1><p><strong>_kafka 版本号为 0.11.0_</strong></p><p>Kafka Consumer 的参数配置如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Map&lt;String, Object&gt; <span class="title">getDefaultConsumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; propsMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 手动设置自动提交为false,交由 spring-kafka 启动的invoker执行提交</span></span><br><span class="line">        propsMap.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        propsMap.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="string">"30000"</span>);</span><br><span class="line">        propsMap.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, <span class="string">"10000"</span>);</span><br><span class="line">        propsMap.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        propsMap.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从partition中获取消息最大大小</span></span><br><span class="line">        propsMap.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, <span class="string">"102400"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> propsMap;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h1 id="Consumer-卡顿现象"><a href="#Consumer-卡顿现象" class="headerlink" title="Consumer 卡顿现象"></a>Consumer 卡顿现象</h1><h2 id="Consumer-卡顿时的日志"><a href="#Consumer-卡顿时的日志" class="headerlink" title="Consumer 卡顿时的日志"></a>Consumer 卡顿时的日志</h2><p>每次卡顿不消费时都出现以下日志：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">2017/11/09 19:35:29:DEBUG pool-16-thread-10 org.apache.kafka.clients.consumer.internals.Fetcher - Fetch READ_UNCOMMITTED at offset 11429299 for partition my_topic-27 returned fetch data (error=NONE, highWaterMark=11429299, lastStableOffset = -1, logStartOffset = 10299493, abortedTransactions = null, recordsSizeInBytes=0)</span><br><span class="line"> 2017/11/09 19:35:29:DEBUG pool-16-thread-10 org.apache.kafka.clients.consumer.internals.Fetcher - Added READ_UNCOMMITTED fetch request for partition my_topic-27 at offset 11429299 to node p-kafka-host-03.ali.keep:9092 (id: 6 rack: null)</span><br><span class="line"> 2017/11/09 19:35:29:DEBUG pool-16-thread-10 org.apache.kafka.clients.consumer.internals.Fetcher - Sending READ_UNCOMMITTED fetch for partitions [my_topic-27] to broker p-kafka-host-03.ali.keep:9092 (id: 6 rack: null)</span><br><span class="line"> 2017/11/09 19:35:29:DEBUG kafka-coordinator-heartbeat-thread | myConsumerGroup org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Sending Heartbeat request for group myConsumerGroup to coordinator p-kafka-host-02:9092 (id: 2147483642 rack: null)</span><br><span class="line">2017/11/09 19:35:29:DEBUG pool-16-thread-13 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Attempt to heartbeat failed for group myConsumerGroup since it is rebalancing.</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-13 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [my_topic-18] for group myConsumerGroup</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-13 org.springframework.kafka.listener.ConcurrentMessageListenerContainer - partitions revoked: [my_topic-18]</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-13 org.springframework.kafka.listener.ConcurrentMessageListenerContainer - partitions revoked: [my_topic-18]</span><br><span class="line">2017/11/09 19:35:29:DEBUG pool-16-thread-4 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Attempt to heartbeat failed for group myConsumerGroup since it is rebalancing.</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-4 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [my_topic-21] for group myConsumerGroup</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-4 org.springframework.kafka.listener.ConcurrentMessageListenerContainer - partitions revoked: [my_topic-21]</span><br><span class="line">2017/11/09 19:35:29:INFO pool-16-thread-4 org.springframework.kafka.listener.ConcurrentMessageListenerContainer - partitions revoked: [my_topic-21]</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">2017/11/09 19:35:29:DEBUG pool-16-thread-4 org.apache.kafka.clients.consumer.internals.Fetcher - Fetch READ_UNCOMMITTED at offset 11426689 for partition my_topic-21 returned fetch data (error=NONE, highWaterMark=11426689, lastStableOffset = -1, logStartOffset = 10552294, abortedTransactions = null, recordsSizeInBytes=0)</span><br><span class="line"> 2017/11/09 19:35:29:DEBUG pool-16-thread-13 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Group myConsumerGroup committed offset 11429849 for partition my_topic-18</span><br><span class="line"> 2017/11/09 19:35:29:INFO pool-16-thread-13 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group myConsumerGroup</span><br><span class="line"> 2017/11/09 19:35:29:DEBUG pool-16-thread-13 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Sending JoinGroup ((type: JoinGroupRequest, groupId=myConsumerGroup, sessionTimeout=30000, rebalanceTimeout=300000, memberId=p-my-consumer-host-03-12-97c12fb0-9bb7-4762-8478-538f06be9e90, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@54371fac)) to coordinator p-kafka-02.ali.keep:9092 (id: 2147483642 rack: null)</span><br></pre></td></tr></table></figure></p><p>其中最重要的部分是：</p><blockquote><p><strong>2017/11/09 19:35:29:DEBUG pool-16-thread-13 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Attempt to heartbeat failed for group myConsumerGroup since it is rebalancing.<br>2017/11/09 19:35:29:INFO pool-16-thread-13 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [my_topic-18] for group myConsumerGroup<br>2017/11/09 19:35:29:INFO pool-16-thread-13 org.springframework.kafka.listener.ConcurrentMessageListenerContainer - partitions revoked: [my_topic-18]<br>…<br>2017/11/09 19:35:29:INFO pool-16-thread-13 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group myConsumerGroup</strong></p></blockquote><p>那为什么每次会这样呢？我们是有单独的线程在发起心跳的！!!</p><h2 id="Consumer-卡顿时的-jstack"><a href="#Consumer-卡顿时的-jstack" class="headerlink" title="Consumer 卡顿时的 jstack"></a>Consumer 卡顿时的 jstack</h2><p>观察日志可以发现，卡顿时 ConsumerCoordinator 在不停地 rejoin group，并且做 rebalance，所以需要对比在正常和卡顿这两种情况下 ConsumerCoordinator 的行为。</p><h3 id="正常时的-ConsumerCoordinator"><a href="#正常时的-ConsumerCoordinator" class="headerlink" title="正常时的 ConsumerCoordinator"></a>正常时的 ConsumerCoordinator</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat jstack.normal.log | grep ConsumerCoordinator -B1 | grep -v ConsumerCoordinator | sort | uniq -c</span><br><span class="line">32 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:931)</span><br><span class="line">22 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:950)</span><br></pre></td></tr></table></figure><h3 id="卡顿时的-ConsumerCoordinator"><a href="#卡顿时的-ConsumerCoordinator" class="headerlink" title="卡顿时的 ConsumerCoordinator"></a>卡顿时的 ConsumerCoordinator</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat jstack.pause.log | grep ConsumerCoordinator -B1 | grep -v ConsumerCoordinator | sort | uniq -c</span><br><span class="line">14 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:316)</span><br><span class="line">14 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:920)</span><br><span class="line">8 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:931)</span><br><span class="line">32 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:950)</span><br></pre></td></tr></table></figure><p>根据以上的现场信息，可以发现关键就在 <code>AbstractCoordinator.ensureActiveGroup</code> 这一步，继续观察 jstack.pause.log 中的相关堆栈信息，如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">"pool-16-thread-14" #167 prio=5 os_prio=0 tid=0x00007f5b19dbf000 nid=0x7ac2 runnable [0x00007f5ae4ccb000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)</span><br><span class="line">        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)</span><br><span class="line">        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)</span><br><span class="line">        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)</span><br><span class="line">        - locked &lt;0x00000000c2e816b0&gt; (a sun.nio.ch.Util$2)</span><br><span class="line">        - locked &lt;0x00000000c2e816a0&gt; (a java.util.Collections$UnmodifiableSet)</span><br><span class="line">        - locked &lt;0x00000000c2e742a0&gt; (a sun.nio.ch.EPollSelectorImpl)</span><br><span class="line">        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)</span><br><span class="line">        at org.apache.kafka.common.network.Selector.select(Selector.java:529)</span><br><span class="line">        at org.apache.kafka.common.network.Selector.poll(Selector.java:321)</span><br><span class="line">        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:433)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:232)</span><br><span class="line">        - locked &lt;0x00000000c2f00da0&gt; (a org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:168)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:364)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:316)</span><br><span class="line">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:297)</span><br><span class="line">        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1078)</span><br><span class="line">        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)</span><br><span class="line">        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:571)</span><br><span class="line">        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">        at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:745)</span><br></pre></td></tr></table></figure></p><h1 id="卡顿原因分析"><a href="#卡顿原因分析" class="headerlink" title="卡顿原因分析"></a>卡顿原因分析</h1><h2 id="卡顿原因：Consumer-在-Region-Group"><a href="#卡顿原因：Consumer-在-Region-Group" class="headerlink" title="卡顿原因：Consumer 在 Region Group"></a>卡顿原因：Consumer 在 Region Group</h2><p>根据以上信息，结合 <code>org.apache.kafka.clients.consumer.internals.ConsumerCoordinator</code> 的代码可以发现在<br><code>ConsumerCoordinator#poll</code> 中判断 <code>needRejoin()</code> 为 true 时会调用 <code>ensureActiveGroup()</code> 函数，如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">long</span> remainingMs)</span> </span>&#123;</span><br><span class="line">        invokeCompletedOffsetCommitCallbacks();</span><br><span class="line">        <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned()) &#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">if</span> (needRejoin()) &#123;</span><br><span class="line">                ...</span><br><span class="line">                ensureActiveGroup();</span><br><span class="line">                ...</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             ...</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        pollHeartbeat(now);</span><br><span class="line">        maybeAutoCommitOffsetsAsync(now);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h2 id="Region-Group-原因：Consumer-Leave-Group"><a href="#Region-Group-原因：Consumer-Leave-Group" class="headerlink" title="Region Group 原因：Consumer Leave Group"></a>Region Group 原因：Consumer Leave Group</h2><p>那么问题就是什么情况下 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#needRejoin 会返回 true，我们还是看看他的实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">needRejoin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!subscriptions.partitionsAutoAssigned())</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we need to rejoin if we performed the assignment and metadata has changed</span></span><br><span class="line">        <span class="keyword">if</span> (assignmentSnapshot != <span class="keyword">null</span> &amp;&amp; !assignmentSnapshot.equals(metadataSnapshot))</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we need to join if our subscription has changed since the last join</span></span><br><span class="line">        <span class="keyword">if</span> (joinedSubscription != <span class="keyword">null</span> &amp;&amp; !joinedSubscription.equals(subscriptions.subscription()))</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">super</span>.needRejoin();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h2 id="kafka-metadata-什么时候变化？？？？"><a href="#kafka-metadata-什么时候变化？？？？" class="headerlink" title="kafka metadata 什么时候变化？？？？"></a>kafka metadata 什么时候变化？？？？</h2><p>可以看到，不是 metadataSnapshot 有变化，也不是 订阅者 subscriptions 有变化，那就是 super.needRejoin() 返回了 true，问题就转到了 <code>org.apache.kafka.clients.consumer.internals.AbstractCoordinator#needRejoin</code> 这个函数，其实现是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">needRejoin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> rejoinNeeded;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从代码上看 <code>rejoinNeeded</code> 的整个变化过程，初始化为 true，在 <code>initiateJoinGroup</code> 成功后，会赋值为 false，在 <code>maybeLeaveGroup</code> 时会赋值为 true，所以怀疑卡顿时是 consumer leave group 了。</p><h2 id="Consumer-Leave-Group-原因：pollTimeoutExpired"><a href="#Consumer-Leave-Group-原因：pollTimeoutExpired" class="headerlink" title="Consumer Leave Group 原因：pollTimeoutExpired"></a>Consumer Leave Group 原因：pollTimeoutExpired</h2><p>在 <code>org.apache.kafka.clients.consumer.internals.AbstractCoordinator.HeartbeatThread#run</code> 中调用了 <code>maybeLeaveGroup()</code> 函数，其实现如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Heartbeat thread for group &#123;&#125; started"</span>, groupId);</span><br><span class="line">                <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                        ...</span><br><span class="line">                        client.pollNoWakeup();</span><br><span class="line">                        <span class="keyword">long</span> now = time.milliseconds();</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> (coordinatorUnknown()) &#123;</span><br><span class="line">                            ...</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (heartbeat.sessionTimeoutExpired(now)) &#123;</span><br><span class="line">                            <span class="comment">// the session timeout has expired without seeing a successful heartbeat, so we should</span></span><br><span class="line">                            <span class="comment">// probably make sure the coordinator is still healthy.</span></span><br><span class="line">                            coordinatorDead();</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (heartbeat.pollTimeoutExpired(now)) &#123;</span><br><span class="line">                            <span class="comment">// the poll timeout has expired, which means that the foreground thread has stalled</span></span><br><span class="line">                            <span class="comment">// in between calls to poll(), so we explicitly leave the group.</span></span><br><span class="line">                            maybeLeaveGroup();</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!heartbeat.shouldHeartbeat(now)) &#123;</span><br><span class="line">                            <span class="comment">// poll again after waiting for the retry backoff in case the heartbeat failed or the</span></span><br><span class="line">                            <span class="comment">// coordinator disconnected</span></span><br><span class="line">                            AbstractCoordinator.<span class="keyword">this</span>.wait(retryBackoffMs);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            heartbeat.sentHeartbeat(now);</span><br><span class="line">                            ...</span><br><span class="line">                        &#125;</span><br><span class="line">                  &#125; <span class="comment">// end synchronized</span></span><br><span class="line">              &#125; <span class="comment">// end while</span></span><br><span class="line">          &#125; <span class="comment">//end try              </span></span><br><span class="line">&#125; <span class="comment">// end run</span></span><br></pre></td></tr></table></figure></p><p>其中最重要的两个 timeout 函数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">sessionTimeoutExpired</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> now - Math.max(lastSessionReset, lastHeartbeatReceive) &gt; sessionTimeout;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">pollTimeoutExpired</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> now - lastPoll &gt; maxPollInterval;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以是 pollTimeoutExpired 引起了 leave group.</p><h2 id="根本原因：pollTimeoutExpired"><a href="#根本原因：pollTimeoutExpired" class="headerlink" title="根本原因：pollTimeoutExpired"></a>根本原因：pollTimeoutExpired</h2><p>pollTimeoutExpired 的原因是两次 poll 的时间间隔超过了设置的 maxPollInterval 值。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>调整以下参数</p><ul><li>max.poll.records：100 (默认值 500)</li><li>max.poll.interval.ms：600000 (默认值 300000，也就是5分钟)</li></ul><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>至此，问题已经解决了，但是有一些疑问。</p><ul><li>对于这两个参数值的设定， 是 <code>max.poll.records</code> 越小越好，<code>max.poll.interval.ms</code> 越大越好吗？</li><li>已经设置过的 <code>session.timeout.ms</code> 和 <code>heartbeat.interval.ms</code>难道没用吗？为什么有这么多超时参数的设置啊？</li><li>已经设置过的 <code>max.partition.fetch.bytes</code> 没用吗？为什么还要设置 <code>max.poll.records</code> 啊？</li><li>整体上还需要调哪些参数才可以让 consumer 运行正常，或者是性能达到最大呢？</li></ul><p>在下一篇博客「Kafka Consumer 的实现」中，将会继续分析 Kafka Consumer 的消费过程和参数配置，试图回答以上问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;运行环境说明&quot;&gt;&lt;a href=&quot;#运行环境说明&quot; class=&quot;headerlink&quot; title=&quot;运行环境说明&quot;&gt;&lt;/a&gt;运行环境说明&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;_kafka 版本号为 0.11.0_&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kafka Cons
      
    
    </summary>
    
    
      <category term="Distributed System" scheme="http://luojinping.com/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 5 广义线性模型</title>
    <link href="http://luojinping.com/2017/11/05/Stanford-Machine-Learning-5-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://luojinping.com/2017/11/05/Stanford-Machine-Learning-5-广义线性模型/</id>
    <published>2017-11-05T15:20:39.000Z</published>
    <updated>2019-08-25T13:10:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="指数分布族-Exponential-Family"><a href="#指数分布族-Exponential-Family" class="headerlink" title="指数分布族(Exponential Family)"></a>指数分布族(Exponential Family)</h1><h2 id="指数分布族的定义"><a href="#指数分布族的定义" class="headerlink" title="指数分布族的定义"></a>指数分布族的定义</h2><p>若一类概率分布可以写成如下形式，那么它就属于指数分布族：</p><script type="math/tex; mode=display">P(y;\eta) = b(y)exp(\eta^TT(y)-a(\eta))</script><ul><li>$\eta$: 自然参数，通常是一个实数</li><li>T(y): 充分统计量，通常，T(y)=y，实际上是一个概率分布的充分统计量（统计学知识）</li><li>a($\eta$) 被称为 log partition function</li></ul><p>对于给定的 a，b，T 三个函数，上式定义了一个以 $\eta$ 为参数的概率分布集合，即改变 $\eta$ 可以得到不同的概率分布，例如高斯分布和伯努利分布。</p><h2 id="指数分布族以及它们的特征"><a href="#指数分布族以及它们的特征" class="headerlink" title="指数分布族以及它们的特征"></a>指数分布族以及它们的特征</h2><ul><li>正态分布（高斯分布）——总体噪音（由中心极限定理得）</li><li>伯努利分布——逻辑回归（对01问题建模）</li><li>多项式分布——K种结果的事情进行建模</li><li>泊松分布——对计数过程进行建模（一个样本中放射性衰变的数目，网站的访客数目，商店的顾客数目）</li><li>伽马分布，指数分布——正数的分布，对间隔进行建模（在公交车站等车的时间）</li><li>β分布，Dirichlet分布——对小数进行分布，对概率分布进行建模</li><li>Wishart分布——协方差的分布</li></ul><h1 id="指数分布簇推导"><a href="#指数分布簇推导" class="headerlink" title="指数分布簇推导"></a>指数分布簇推导</h1><p>高斯分布(Gaussian)和伯努利(Bernoulli)分布都可以推导为指数分布族。</p><h2 id="伯努利分布的推导"><a href="#伯努利分布的推导" class="headerlink" title="伯努利分布的推导"></a>伯努利分布的推导</h2><p>伯努利分布的概率公式为：$P(y=1;\phi)=\phi; P(y=0;\phi)=1-\phi;$</p><p>公式可经如下变换：</p><script type="math/tex; mode=display">P(y;\phi)=\phi^y(1-\phi)^y</script><script type="math/tex; mode=display">=exp(log(\phi^y(1-\phi)^y))=exp(ylog(\phi)+ (1-y)log(1-\phi))</script><script type="math/tex; mode=display">=exp(log(\frac\phi{1-\phi})y + log(1-\phi))</script><p>对应的指数分布族的参数为：<br>$T(y) = y$<br>$b(y) = 1$<br>$\eta = log(\frac\phi{1-\phi}) =&gt; \phi=\frac1{1+e^{-n}}$<br>$a(\eta) = -log(1-\phi) = log(1+e^n)$</p><h2 id="高斯分布的推导"><a href="#高斯分布的推导" class="headerlink" title="高斯分布的推导"></a>高斯分布的推导</h2><p>在线性回归中，$\sigma$ 对于模型参数 $\theta$  的选择没有影响，为了推导方便我们令 $\sigma = 1$。<br>则有：</p><script type="math/tex; mode=display">P(y;\mu)=\frac{1}{\sqrt{2\pi}}exp(-\frac12(y-\mu)^2)</script><script type="math/tex; mode=display">=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}y^2) * exp({\mu}y-\frac{1}{2}\mu^2)</script><p>对应的指数分布族的参数为：<br>$T(y) = y$<br>$b(y) = \frac{1}{\sqrt{2\pi}}exp(-\frac12y^2)$<br>$\eta = \mu$<br>$a(\eta) = \frac{ {\mu}^2}2 = \frac{ {\eta}^2}2$</p><h1 id="广义线性模型-Generalized-Linear-Model"><a href="#广义线性模型-Generalized-Linear-Model" class="headerlink" title="广义线性模型(Generalized Linear Model)"></a>广义线性模型(Generalized Linear Model)</h1><p>想用 广义线性模型对一般问题进行建模首先需要明确几个 假设：</p><ol><li>$y | x;θ \sim ExponentialFamily(\eta)$ y的条件概率属于指数分布族;</li><li>给定 x 广义线性模型的目标是求解 T(y) | x， 不过由于 很多情况下 T(y) = y  所以我们的目标变成了 y | x , 也即 我们希望拟合函数为 h(x) = E[y|x] (这个条件在线性回归和逻辑回归中都满足， 例如在逻辑回归中 $hθ(x) = p(y = 1|x;\theta) = 0 \cdot p(y = 0|x; \theta) + 1 \cdot  p(y = 1|x; \theta) = E[y|x;\theta])$</li><li>自然参数 $\eta$ 与 x 是线性关系：$\eta=\theta^Tx$ ($\eta 为向量时 \eta_{i} = \theta_{i}^Tx$)</li></ol><p>有了如上假设，就可以进行建模和求解了。</p><p>对于伯努利分布，可以推导出：<br>…<br>这也就是逻辑回归中 sigmod 函数的由来。</p><h1 id="多分类算法-Softmax-Regression"><a href="#多分类算法-Softmax-Regression" class="headerlink" title="多分类算法(Softmax Regression)"></a>多分类算法(Softmax Regression)</h1><p>y有多个可能的分类：{1, 2, …, k}</p><p>=======具体的公式略=======</p><p>最后求借寻找最佳参数时，跟最小二乘和逻辑回归的解法类似，可以用梯度下降法或者牛顿迭代法。</p><h1 id="Referecen"><a href="#Referecen" class="headerlink" title="Referecen"></a>Referecen</h1><p><a href="https://zhuanlan.zhihu.com/p/22876460" target="_blank" rel="noopener">广义线性模型(Generalized Linear Model)</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;指数分布族-Exponential-Family&quot;&gt;&lt;a href=&quot;#指数分布族-Exponential-Family&quot; class=&quot;headerlink&quot; title=&quot;指数分布族(Exponential Family)&quot;&gt;&lt;/a&gt;指数分布族(Exponen
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 4 逻辑回归</title>
    <link href="http://luojinping.com/2017/11/05/Stanford-Machine-Learning-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://luojinping.com/2017/11/05/Stanford-Machine-Learning-4-逻辑回归/</id>
    <published>2017-11-05T15:06:43.000Z</published>
    <updated>2019-08-25T13:10:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>对于逻辑回归而言，y 的取值不是 0 就是 1，所以 $h_θ(x)$ 可以写为</p><script type="math/tex; mode=display">h_θ(x) = g(θ^{T}x)=\frac1{1+e^{-θ^{T}x}}</script><p>其中</p><script type="math/tex; mode=display">g(z)=\frac1{1+e^{-z}}$$；g(z) 被称为 logistic function 或 sigmoid function，其二维坐标下的曲线为:![sigmoid function](/img/sigmoid_function.png)我们先取 g(z) 为 sigmoid function，如果有其他使得 y 值从 0 到 1 平滑递增的函数也可以使用。但由于一些列原因(在后续的一般化回归模型 GLM 中会谈到为什么选用这个函数)，g(z) is a fairly natural one.g(z) 的导数我们可以先进行推导:$$g'(z)=\frac{d}{dz}\frac{1}{1+e^{-z}}= \frac{1}{(1+e^{-z})^2}(e^{-z})</script><script type="math/tex; mode=display">= \frac{1}{1+e^{-z}}*(1 - \frac{1}{1+e^{-z}})= g(z)(1-g(z))</script><h1 id="梯度上升法求解逻辑回归"><a href="#梯度上升法求解逻辑回归" class="headerlink" title="梯度上升法求解逻辑回归"></a>梯度上升法求解逻辑回归</h1><p>对于给定的逻辑回归函数，我们使用最小二乘法来推导出最大似然估计，假设:<br>$P(y=1|x;θ)=h_θ(x)$，代表对于给定的 θ，y 取值为 1 的概率。<br>$P(y=0|x;θ)=1-h_θ(x)$，代表对于给定的 θ，y 取值为 0 的概率。</p><p>以上两者可以合并为：</p><script type="math/tex; mode=display">P(y|x;θ)=(h_θ(x))^y(1 − h_θ(x))^{(1−y)}</script><p>假设 m 个训练集是相互独立的，则似然估计为：</p><script type="math/tex; mode=display">L(θ)=P(\overrightarrow{y}|X;θ)</script><script type="math/tex; mode=display">= \prod^m_{i=1}P(y^i|x^i;θ)</script><script type="math/tex; mode=display">= \prod^m_{i=1}{(h_θ(x^{(i)}))^{y^{(i)}}(1 − h_θ(x^{(i)}))^{(1−y^{(i)})}}</script><p>和之前一样，上式可以简化为：</p><p></p><p><br>$l(θ) = logL(θ)<br>= \sum_{m}^{i=1}{y^{(i)}}log{h(x^{(i)}) + {(1−y^{(i)})}log(1 − h(x^{(i)}))}$</p><p>那么，<br>如何去最大化似然函数呢，可以应用梯度上升法，因为我们要使 P 的取值足够大，也是就预测准确的概率最够大。</p><p>随机梯度上升的公式为：</p><script type="math/tex; mode=display">θ:= θ + \alpha\Deltaθl(θ)</script><p>下面来求$\Deltaθl(θ)$的取值：</p><script type="math/tex; mode=display">\frac\partial{\partial\theta_j}l(\theta)</script><script type="math/tex; mode=display">= (y\frac1{g(\theta^Tx)} - (1-y)\frac1{1-g(\theta^Tx)})\frac\partial{\partial\theta_j}g(\theta^Tx)</script><script type="math/tex; mode=display">= (y\frac1{g(\theta^Tx)} - (1-y)\frac1{1-g(\theta^Tx)}) g(\theta^Tx)(1-g(\theta^Tx))\frac\partial{\partial\theta_j}\theta^Tx</script><script type="math/tex; mode=display">= ({y(1-g(\theta^Tx))-(1-y)g(\theta^Tx)})x_j</script><script type="math/tex; mode=display">= (y - h_{\theta}(x))x_j</script><p>附上手写的推导过程：<br><img src="/img/logistic_regression_delta_ljp_derived.png" alt="手写推导过程"></p><p>所以，最终随机梯度上升的公式为：</p><script type="math/tex; mode=display">θ_j:=θ_j + \alpha\sum_{i=1}^{m}(y^{(i)} - h_{\theta}(x^{(i))})x_j^{(i)}</script><p>如何和线性回归的公式放在一起比较，</p><script type="math/tex; mode=display">θ_j = θ_j - α \frac1m \* \sum_{i=1}^{m}{(h_θ(x^{(i)}) - y^{(i)})}\*x_j^{(i)}</script><p>会发现，这两者非常相似，实际上却不然，因为这里的 $(h_θ(x^{(i)})$ 定义的不是线性函数。后续我们谈到 GLM 时会发现这并不是巧合，而是有更深层次的原因。</p><h1 id="牛顿迭代法求解逻辑回归"><a href="#牛顿迭代法求解逻辑回归" class="headerlink" title="牛顿迭代法求解逻辑回归"></a>牛顿迭代法求解逻辑回归</h1><p>牛顿迭代法可以利用到曲线本身的信息，比梯度下降法更容易收敛，即迭代更少次数。</p><h2 id="牛顿迭代法简述"><a href="#牛顿迭代法简述" class="headerlink" title="牛顿迭代法简述"></a>牛顿迭代法简述</h2><p>假设我们要求解方程 f(x)=0 的根，首先随便找一个初始值 x0，如果 x0 不是解，做一个经过 (x0,f(x0))  这个点的切线，与 x 轴的交点为 x1。同样的道理，如果 x1 不是解，做一个经过 (x1,f(x1)) 这个点的切线，与 x 轴的交点为 x2。 以此类推。以这样的方式得到的 xi 会无限趋近于 f(x)=0 的解。</p><p>对于任意一点 $(x_n,y_n)$ 做切线，切线的斜率为 $f’(x_n)$，则有方程：</p><script type="math/tex; mode=display">y-f(x_n) = f'(x_n)(x-x_n)</script><h2 id="迭代过程"><a href="#迭代过程" class="headerlink" title="迭代过程"></a>迭代过程</h2><p>求解 $f(\theta)$ = 0 时 $\theta$ 的取值。<br>设下一次迭代时 $\theta^{(t+1)}$ 的取值与前一次迭代 $\theta^{(t)}$ 的取值(在 x 轴)距离为 $\Delta$。</p><p>则 $\theta^{(t+1)} = \theta^{(t)} - \Delta$，且 $\Delta = \frac{f(\theta^{(t)})}{f’(\theta^{(t)})}$，<br>所以有：</p><script type="math/tex; mode=display">\theta^{(t+1)} = \theta^{(t)} - \frac{f(\theta^{(t)})}{f'(\theta^{(t)})}</script><h3 id="从泰勒展开到牛顿迭代"><a href="#从泰勒展开到牛顿迭代" class="headerlink" title="从泰勒展开到牛顿迭代"></a>从泰勒展开到牛顿迭代</h3><p>也可以由泰勒展开中推导牛顿迭代的公式。这次为了求解方程 f′=0 的根，把原函数 f(x) 的做泰勒展开，展开到二阶形式： </p><script type="math/tex; mode=display">f(x+\Delta x) = f(x)+f'(x)\Delta x+ \frac1{2}f''(x)\Delta x^2</script><p>当且仅当 $\Delta x$ 逼近 0 时，上式成立，此时忽略 1/2 系数的作用，所以有：</p><script type="math/tex; mode=display">f'(x)+ \frac1{2}f''(x)\Delta x = 0</script><p>故：</p><script type="math/tex; mode=display">\Delta x = -\frac{f'(x)}{f''(x)}</script><p><strong>对函数求极大值的方法</strong><br>&gt;</p><blockquote><ol><li>将原函数y=f(x)，对x求一次导数，得到dy/dx；</li><li>令dy/dx = 0，解得一次导函数的零点；</li><li>将原函数对x求二次导函数；</li><li>将解得的零点坐标的x值代入二次导函数，<br>如果是正值，零点所在位置，就是极小值点，再将该x值代入原函数，得到极小值；<br>如果是值值，零点所在位置，就是极大值点，再将该x值代入原函数，得到极大值；<br>如果是0，零点所在位置，既不是极小值点，也不是极大值点，是拐点。</li></ol></blockquote><p>所以求 $l(\theta)$ 在极大值处 $\theta$ 的取值，则是求 $l’(\theta) = 0$ 时 $\theta$ 的值，应用牛顿迭代法则有：</p><script type="math/tex; mode=display">\theta^{(t+1)} = \theta^{(t)} - \frac{l'(\theta^{(t)})}{l''(\theta^{(t)})}</script><h2 id="多维向量的牛顿迭代"><a href="#多维向量的牛顿迭代" class="headerlink" title="多维向量的牛顿迭代"></a>多维向量的牛顿迭代</h2><p>对于多维向量 $\overrightarrow{X}$ 求解。</p><script type="math/tex; mode=display">\theta := \theta - H^{-1} \nabla l(\theta)</script><p>其中<br>$\nabla l(\theta)$ 是对 $l(\theta)$ 求导的值。</p><p>H 是一个 n*n 的矩阵，n 是特征数量，元素的计算公式为：</p><script type="math/tex; mode=display">H_ij= \frac{\partial^2{l({\theta)}}}{\partial{\theta_i}\partial{\theta_j}}</script><h2 id="牛顿迭代法的特点"><a href="#牛顿迭代法的特点" class="headerlink" title="牛顿迭代法的特点"></a>牛顿迭代法的特点</h2><h3 id="是否收敛"><a href="#是否收敛" class="headerlink" title="是否收敛"></a>是否收敛</h3><p>通常情况下是收敛的，但是需要满足一些条件，对于逻辑回归来讲，是收敛的。</p><h3 id="迭代速度"><a href="#迭代速度" class="headerlink" title="迭代速度"></a>迭代速度</h3><p>每次迭代后，有解数字的误差是成平方倍减小的，是二次收敛函数。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：收敛快<br>缺点：特征多(上千个)时，每次迭代成本大</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://blog.csdn.net/baimafujinji/article/details/51179381" target="_blank" rel="noopener">http://blog.csdn.net/baimafujinji/article/details/51179381</a><br><a href="http://blog.csdn.net/baimafujinji/article/details/51167852" target="_blank" rel="noopener">http://blog.csdn.net/baimafujinji/article/details/51167852</a><br><a href="https://www.jiqizhixin.com/articles/2017-08-09-3" target="_blank" rel="noopener">如何通过牛顿方法解决Logistic回归问题</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;逻辑回归&quot;&gt;&lt;a href=&quot;#逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归&quot;&gt;&lt;/a&gt;逻辑回归&lt;/h1&gt;&lt;p&gt;对于逻辑回归而言，y 的取值不是 0 就是 1，所以 $h_θ(x)$ 可以写为&lt;/p&gt;
&lt;script type=&quot;mat
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 3 线性回归的概率解释</title>
    <link href="http://luojinping.com/2017/11/05/Stanford-Machine-Learning-3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/"/>
    <id>http://luojinping.com/2017/11/05/Stanford-Machine-Learning-3-线性回归的概率解释/</id>
    <published>2017-11-05T14:04:40.000Z</published>
    <updated>2019-08-25T13:10:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h1><p>欠拟合：underfitting，与训练数据贴合的不够好，不能准确预测未来目标值。<br>过拟合：overfitting，与训练数据贴合的太好了，预测未来目标值的准确性有较大风险。</p><p><img src="/img/LR_underfitting_and_overfitting.png" alt></p><h1 id="线性模型的概率解释"><a href="#线性模型的概率解释" class="headerlink" title="线性模型的概率解释"></a>线性模型的概率解释</h1><p>思考：我们为什么要用最小二乘的指标作为 cost function？为什么不是绝对值或四次方？</p><blockquote><p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。<br>最小二乘是从函数形式上来看的，极大似然是从概率意义上来看的。事实上，最小二乘可以由高斯噪声假设+极大似然估计推导出来。当然极大似然估计还可以推导出其他的loss function， 比如logistic回归中，loss function是交叉熵。<br><a href="http://www.cnblogs.com/little-YTMM/p/5700226.html" target="_blank" rel="noopener">最大似然估计与最小二乘估计的区别</a></p></blockquote><p>一般的最小二乘法实际上是在假设误差项满足高斯分布且独立同分布的情况下，使似然性最大化。</p><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>回到预测房价的例子，假设最终的预测函数，每一次预测都有误差，用$ε^{(i)}$表示误差，则预测函数可以写为：</p><script type="math/tex; mode=display">y^{(i)}=\theta^Tx^{(i)} + ε^{(i)}</script><p>其中，误差是随机分布的，均值为 0，服从高斯分布 $N(0,σ^2)$。</p><blockquote><p>Andrew Ng 讲到在大多数情况下，线性回归的误差值如果综合来看，就是符合高斯分布的。并且根据中心极限定律，正态分布确实是对误差项分布的合理猜想。</p></blockquote><p>所以</p><script type="math/tex; mode=display">P(y^{(i)}|x^{(i)}; θ) = \frac{1}{\sqrt{2\pi}\sigma}exp(- \frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})</script><p>$P(y^{(i)}|x^{(i)}; θ)$ 表示：在 θ 为给定的参数的情况下，概率 $y^{(i)}$ 以 $x^{(i)}$ 为随机变量的概率分布，注意 θ 不是随机变量。</p><p>由于 ε(i) 是独立的同分布（IID：independentlyidentically distribution），所以以 θ 为变量的似然函数为：</p><script type="math/tex; mode=display">L(θ)=L(θ;X,Y)=p(Y|X;θ) = \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(- \frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})</script><p>对 L(θ) 取对数有：</p><script type="math/tex; mode=display">l(\theta)=logL(\theta)= log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(- \frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})</script><script type="math/tex; mode=display">= m\sum_{i=1}^{m}log\frac{1}{\sqrt{2\pi}\sigma} - \frac1{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2</script><p>最大化 $l(\theta)$ 即是最小化 $\frac1{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$，这样就是 cost function.</p><p>由于目标变量服从正态分布，但分布的均值和方差都未知，对均值和方差两个参数的合理估计是选取两个参数使得在正态分布的前提下，抽到各样本中的 y 值的概率最大，这就是最大似然估计的思想。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://www.holehouse.org/mlclass/07_Regularization.html" target="_blank" rel="noopener">http://www.holehouse.org/mlclass/07_Regularization.html</a><br><a href="http://rstudio-pubs-static.s3.amazonaws.com/4810_06e3d8fd26ed40eb8c31aff35eae81ae.html" target="_blank" rel="noopener">http://rstudio-pubs-static.s3.amazonaws.com/4810_06e3d8fd26ed40eb8c31aff35eae81ae.html</a><br><a href="https://rpubs.com/badbye/ml03" target="_blank" rel="noopener">https://rpubs.com/badbye/ml03</a><br><a href="http://www.qiujiawei.com/linear-algebra-15/" target="_blank" rel="noopener">http://www.qiujiawei.com/linear-algebra-15/</a><br><a href="http://www.jianshu.com/p/f1d3906e4a3e" target="_blank" rel="noopener">最大似然估计</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;欠拟合与过拟合&quot;&gt;&lt;a href=&quot;#欠拟合与过拟合&quot; class=&quot;headerlink&quot; title=&quot;欠拟合与过拟合&quot;&gt;&lt;/a&gt;欠拟合与过拟合&lt;/h1&gt;&lt;p&gt;欠拟合：underfitting，与训练数据贴合的不够好，不能准确预测未来目标值。&lt;br&gt;过拟合：o
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 2 线性回归进阶</title>
    <link href="http://luojinping.com/2017/11/05/Stanford-Machine-Learning-2-Linear-Regression-with-multiple-features/"/>
    <id>http://luojinping.com/2017/11/05/Stanford-Machine-Learning-2-Linear-Regression-with-multiple-features/</id>
    <published>2017-11-05T14:00:04.000Z</published>
    <updated>2019-08-25T13:08:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多变量的线性回归"><a href="#多变量的线性回归" class="headerlink" title="多变量的线性回归"></a>多变量的线性回归</h1><p>n: 特征(features) 数量<br>m: 训练集数量<br>$x^{(i)}$: </p><ul><li>表示一条训练数据的向量</li><li>i is an index into the training set</li><li>So <ul><li>x is an n-dimensional feature vector</li><li>$x^{(3)}$ is, for example, the 3rd training data</li></ul></li></ul><p>$x^{(j)}_i$: The value of feature j in the ith training example</p><p>例如，当 n=4 时:</p><script type="math/tex; mode=display">h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + θ_3x_3 + θ_4x_4</script><p>For convenience of notation, $x_0$ = 1, 所以最后的特征向量的维度是 n+1，从 0 开始，记为”X”，<br>则有：</p><script type="math/tex; mode=display">h_θ(x)=θ^TX</script><p>$θ^T$: [1 * (n+1)] matrix</p><h1 id="多变量的梯度下降"><a href="#多变量的梯度下降" class="headerlink" title="多变量的梯度下降"></a>多变量的梯度下降</h1><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><script type="math/tex; mode=display">J(θ_0, θ_1, ...,θ_n) =  \frac1{2m}\sum_{i=1}^{m}{(h_θ(x^{(i)}) - y^{(i)})^2}</script><h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p>Repeat {</p><script type="math/tex; mode=display">θ_j = θ_j - α\frac\partial{\partial J(θ_0, θ_1, ...,θ_n)}</script><p>}</p><p>every iterator</p><ul><li>θj = θj - learning rate (α) times the partial derivative of J(θ) with respect to θJ(…)</li><li>We do this through a simultaneous update of every θj value</li></ul><script type="math/tex; mode=display">\frac\partial{\partial J(θ_0, θ_1, ...,θ_n)}</script><script type="math/tex; mode=display">= \frac1m * \sum_{i=1}^{m}{(h_θ(x^{(i)}) - y^{(i)})}*x_j^{(i)}</script><h1 id="Gradient-Decent-in-practice"><a href="#Gradient-Decent-in-practice" class="headerlink" title="Gradient Decent in practice"></a>Gradient Decent in practice</h1><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p>假设只有 $x_1$,$x_2$ 两个变量，其中：$x_1\in(0,2000), x_2\in(1,5)$，则最后的 J(θ) 图形是一个椭圆，在椭圆下用梯度下降法会比圆形要耗时更久，So we need to rescale this input so it’s more effective，有很多方式，一种是将各个 feature 除以其本身的最大值，缩小范围至[0,1]，一种是各个 feature 减去 mean 然后除以最大值，缩小范围至[-0.5,0.5]</p><h2 id="Learning-Rate-α"><a href="#Learning-Rate-α" class="headerlink" title="Learning Rate α"></a>Learning Rate α</h2><ul><li>working correctly: If gradient descent is working then J(θ) should decrease after every iteration</li><li>convergence: 收敛是指每经过一次迭代，J(θ)的值都变化甚小。</li><li>choose α<ol><li>When to use a smaller α<ul><li>If J(θ) is increasing, see below picture</li><li>If J(θ) looks like a series of waves, decreasing and increasing again</li><li>But if α is too small then rate is too slow</li></ul></li><li>Try a range of α values<ul><li>Plot J(θ) vs number of iterations for each version of alpha</li><li>Go for roughly threefold increases: 0.001, 0.003, 0.01, 0.03. 0.1, 0.3</li></ul></li></ol></li></ul><p><img src="/img/gradient_descent_plot.jpg" alt></p><h1 id="Features-and-polynomial-regression"><a href="#Features-and-polynomial-regression" class="headerlink" title="Features and polynomial regression"></a>Features and polynomial regression</h1><h2 id="Can-create-new-features"><a href="#Can-create-new-features" class="headerlink" title="Can create new features"></a>Can create new features</h2><p>如何选择 features 和表达式尤为关键，例如房价与房子的长，房子的宽组成的表达式就会麻烦很多，若将房子的长乘以房子的宽得出面积，则有房价与房子面积的表达式，将会更容易拟合出房价的走势。</p><h2 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h2><p>例如房价的走势，如下图，横坐标 x 为房子的面积，纵坐标为房价，使用一元二次的方程，会得出下图的蓝色曲线。容易得到房价今后会有一个下降的过程，可实际上房价是不会随着面积的增大而下降的。所以需要重新选定 Polynomial regression，可以改为使用一元三次的方程或者使用平凡根的方程。</p><p><strong>所以选择合适的 Features 和 Polynomial regression 都非常重要。</strong></p><p><img src="/img/polynomial_regression_choose.jpg" alt></p><h1 id="Normal-equation-求解多变量线性回归"><a href="#Normal-equation-求解多变量线性回归" class="headerlink" title="Normal equation 求解多变量线性回归"></a>Normal equation 求解多变量线性回归</h1><h2 id="Normal-equation"><a href="#Normal-equation" class="headerlink" title="Normal equation"></a>Normal equation</h2><p>举例说明，假设 J(θ) 是一元二次方程，如：J(θ)=a$θ^2$+bθ+c，则令 <script type="math/tex">\frac{d}{dθ}J(θ)=2aθ+b=0</script> 即可，求出最终的 θ 则得到了线性回归方程，可以预测出今后的 y 值。</p><p>更普遍地，当 θ 是一个 n+1 维的向量时，θ $\in$ $R^{n+1}$，则 cost function 如下：</p><script type="math/tex; mode=display">J(θ_0, θ_1, ...,θ_n) =  \frac1{2m}\sum_{i=1}^{m}{(h_θ(x^{(i)}) - y^{(i)})^2}</script><p>只需要令：</p><script type="math/tex; mode=display">\frac\partial{\partial θ_j}J(θ_0, θ_1, ...,θ_n) = ... = 0 $$，其中 j = 0,1,2,...,n设 X 代表训练集的 features 的值的矩阵，y 代表训练集的结果的值的矩阵，假设训练集数量为 m, features 个数为 n, 则 X 为 (m\*n) 的矩阵，y 为 (m\*1) 的矩阵，可以推导出求 θ 向量的公式如下：$$θ = (X^TX)^{-1}X^Ty</script><h1 id="Gradient-descent-Vs-Normal-equation"><a href="#Gradient-descent-Vs-Normal-equation" class="headerlink" title="Gradient descent Vs Normal equation"></a>Gradient descent Vs Normal equation</h1><h2 id="Gradient-descent-1"><a href="#Gradient-descent-1" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><ul><li>Need to chose learning rate</li><li>Needs many iterations - could make it slower</li><li>Works well even when n is massive (millions)</li><li>Better suited to big data</li><li>What is a big n though: 100 or even a 1000 is still (relativity) small, If n is 10000 then look at using gradient descent</li><li>适用于线性回归会逻辑回归</li></ul><h2 id="Normal-equation-1"><a href="#Normal-equation-1" class="headerlink" title="Normal equation"></a>Normal equation</h2><ul><li>No need to chose a learning rate</li><li>No need to iterate, check for convergence etc.</li><li>Normal equation needs to compute $(X^TX)^{-1}$<ul><li>This is the inverse of an n x n matrix</li><li>With most implementations computing a matrix inverse grows by O(n3), So not great</li></ul></li><li>Slow of n is large, Can be much slower</li><li>仅适用于线性回归</li></ul><h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>局部加权回归(locally weighted regression)简称 loess，其思想是，针对对某训练数据的每一个点，选取这个点及其临近的一批点做线性回归；同时也需要考虑整个训练数据，考虑的原则是距离该区域越近的点贡献越大，反之则贡献越小，这也正说明局部的思想。其 cost function 为：</p><script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m} w^{(i)}( y^{(i)}-\theta^Tx^{(i)} )^2</script><p>其中</p><script type="math/tex; mode=display">w^{(i)} =  exp (-\frac{(x^{(i)}-x)^2}{\tau^2})</script><p>$w^{(i)}$的形式跟正态分布很相似，但二者没有任何关系，仅仅只是便于计算。可以发现，$x^{(j)}$ 离 $x^{(i)}$ 非常近时，${w^{(i)}_j}$ 的值接近于1，此时 j 点的贡献很大，当 $x^{(j)}$ 离 $x^{(i)}$ 非常远时，${w^{(i)}_j}$ 的值接近于 0，此时 j 点的贡献很小。</p><p>$\tau^2$ 是波长函数(bandwidth)， 控制权重随距离下降的速度，τ 越小则 x 离 $x^{(i)}$ 越远时 $w^{(i)}$ 的值下降的越快。</p><p>所以，如果沿着 x 轴的每个点都进行局部直线拟合，那么你会发现对于这个数据集合来说，局部加权的预测结果，能够最终跟踪这条非线性的曲线。</p><p>但局部加权回归也有其缺点：</p><ul><li>每次对一个点的预测都需要整个数据集的参与，样本量大且需要多点预测时效率低。提高效率的方法参考 Andrew More’s KD Tree</li><li>不可外推，对样本所包含的区域外的点进行预测时效果不好，事实上这也是一般线性回归的弱点</li></ul><p>对于线性回归算法，一旦拟合出适合训练数据的参数θ，保存这些参数θ，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。</p><p>对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数θ），没有固定的参数θ，所以是非参数算法(non-parametric algorithm)。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p><a href="http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html" target="_blank" rel="noopener">http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;多变量的线性回归&quot;&gt;&lt;a href=&quot;#多变量的线性回归&quot; class=&quot;headerlink&quot; title=&quot;多变量的线性回归&quot;&gt;&lt;/a&gt;多变量的线性回归&lt;/h1&gt;&lt;p&gt;n: 特征(features) 数量&lt;br&gt;m: 训练集数量&lt;br&gt;$x^{(i)}$: &lt;
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stanford Machine Learning - 1 线性回归入门</title>
    <link href="http://luojinping.com/2017/11/05/Stanford-Machine-Learning-1-Linear%20Regression%20with%20One%20Variable/"/>
    <id>http://luojinping.com/2017/11/05/Stanford-Machine-Learning-1-Linear Regression with One Variable/</id>
    <published>2017-11-05T13:55:04.000Z</published>
    <updated>2019-08-25T13:07:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本系列的课程来源是 <a href="https://see.stanford.edu/Course/CS229" target="_blank" rel="noopener">斯坦福大学公开课 CS229: 机器学习课程</a>，也可以看<a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课的资源</a>，是带字幕的。斯坦福的 CS229 课程相比于 Course 上的 <a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning</a> 课程，理论更强，讲解的也更深入，需要有一些的高数基础。两个课程都看了前半部分，更推荐前者，所以相关笔记对应的都是 CS229 课程。</p><h1 id="线性回归的定义"><a href="#线性回归的定义" class="headerlink" title="线性回归的定义"></a>线性回归的定义</h1><p>适用于监督学习，根据已有的数据集合(x, y)，来推断出将来的数据趋势。</p><h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><p>最后的函数应该是 y = ax + b，假设 hypothesis 为：</p><script type="math/tex; mode=display">h_{\theta}(x) = \theta_{0} + \theta_{1}</script><p>则问题转化为求 $\theta_{0}$ 和 $\theta_{1}$ 的值。要求这两个值需要转化上式，并根据已有的数据来求解。下面介绍损失函数，又叫代价函数的概念。</p><h1 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h1><p>针对每一组数据，公式的值是 $h_{\theta}$($x_{i}$), 实际的值是 $y_{i}$，我们要达到的效果则是公式能够尽量表达已有的 m 组数据集合，即 $( h_{\theta}(x^{(i)}) - y_{i})^{2}$ 的值尽量小。<br>所以，对于所有数据集合，需要求使得 </p><script type="math/tex; mode=display">\frac1{2m}\sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}$$ 最小的 $\theta$ 值。上式又称为 Cost Function，可以写为：$$ J(\theta_0, \theta_1) =  \frac1{2m}\sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}</script><p>我们需要最小化这个 Cost Function。</p><h2 id="Cost-Function-的作用"><a href="#Cost-Function-的作用" class="headerlink" title="Cost Function 的作用"></a>Cost Function 的作用</h2><p>假设 $\theta_0$ = 0，则有 $\theta_1$ 和 J($\theta_1$) 的关系，且图形如下：<br><img src="/img/cost_function_theta1.jpg" alt></p><p>所以当 $\theta_1$ = 1 时，</p><script type="math/tex; mode=display">J(\theta_1)= \frac1{2m}\sum_{i=1}^{m}{(\theta_1x^{(i)} - y^{(i)})^2}</script><p>很容易看出，$J(\theta_1)$ 是关于 $\theta_1$ 的一元二次方程，对于所有的训练数据，每个 $\theta_1$ 的取值都会得到一个 $J(\theta_1)$ 值，而 $J(\theta_1)$ 和 $\theta_1$ 的对应关系根据一元二次方程可知，函数曲线如上图。<br>当 $J(\theta_1)$ 最小时，求得 $\theta_1$ 结果。</p><p>当 $\theta_0$ 和 $\theta_1$ 都不为 0 时，J($\theta_0$, $\theta_1$) 的图形如下：<br><img src="/img/cost_function_theta0_theta1.jpg" alt></p><p>对于两个系数的情况不如一个系数是一个二维坐标系的抛物线那么简单。下面将介绍梯度下降法。</p><h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><ul><li>Start with initial guesses</li><li>Start at 0,0 (or any other value)</li><li>Keeping changing $\theta_0$ and $\theta_1$ a little bit to try and reduce J($\theta_0$, $\theta_1$)</li><li>Each time you change the parameters, you select the gradient which reduces J($\theta_0$, $\theta_1$) the most possible </li><li>Repeat</li><li>Do so until you converge to a local minimum<br>Has an interesting property<ul><li>Where you start can determine which minimum you end up</li><li>Here we can see one initialization point led to one local minimum</li><li>The other led to a different one</li></ul></li></ul><p><img src="/img/gradient_descent_progress.jpg" alt></p><h2 id="具体的计算过程"><a href="#具体的计算过程" class="headerlink" title="具体的计算过程"></a>具体的计算过程</h2><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j}J(\theta_0, \theta_1)</script><p>(for j = 0 and j = 1)</p><h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><p><strong>$\alpha$</strong></p><ul><li>Is a number called the learning rate</li><li>Controls how big a step you take<ul><li>If α is big have an aggressive gradient descent</li><li>If α is small take tiny steps</li></ul></li><li>Too small<ul><li>Take baby steps</li><li>Takes too long</li></ul></li><li>Too large<ul><li>Can overshoot the minimum and fail to converge</li></ul></li></ul><h2 id="Computer"><a href="#Computer" class="headerlink" title="Computer"></a>Computer</h2><p>每次都是<strong>同时</strong>计算 $\theta_0, \theta_1$ 的值，如下：</p><script type="math/tex; mode=display">temp0:= \theta_0 - \alpha \frac\partial{\partial\theta_0}J(\theta_0, \theta_1)</script><script type="math/tex; mode=display">temp1:= \theta_1 - \alpha \frac\partial{\partial\theta_1}J(\theta_0, \theta_1)</script><script type="math/tex; mode=display">\theta_0 := temp0</script><script type="math/tex; mode=display">\theta_1 := temp1</script><p><img src="/img/gradient_descent_demo.jpg" alt></p><h3 id="利用梯度下降法求解线性回归问题"><a href="#利用梯度下降法求解线性回归问题" class="headerlink" title="利用梯度下降法求解线性回归问题"></a>利用梯度下降法求解线性回归问题</h3><script type="math/tex; mode=display">\frac\partial{\partial\theta_j}J(\theta_0, \theta_1)</script><script type="math/tex; mode=display">=\frac\partial{\partial\theta_j} * \frac1{2m}\sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}</script><script type="math/tex; mode=display">=\frac\partial{\partial\theta_j} * \frac1{2m}\sum_{i=1}^{m}{(\theta_0 +\theta_1x^{(i)} - y^{(i)})^2}</script><p>对于 j = 0 or 1 的情况有：<br>j = 0:</p><script type="math/tex; mode=display">\frac\partial{\partial\theta_0}J(\theta_0, \theta_1) = \frac1{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})</script><p>j = 1:</p><script type="math/tex; mode=display">\frac\partial{\partial\theta_1}J(\theta_0, \theta_1) = \frac1{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})*x^{(i)}</script><h2 id="梯度下降法的证明"><a href="#梯度下降法的证明" class="headerlink" title="梯度下降法的证明"></a>梯度下降法的证明</h2><p>1、如果优化函数存在解析解。例如我们求最值一般是对优化函数求导，找到导数为0的点。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数。</p><p>2、如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，互相依赖的情况。或者求导后式子得不到解释解，或者未知参数的个数大于方程组的个数等。这时候使用迭代算法来一步一步找到最优解。</p><ul><li>当目标函数是凸函数时，梯度下降法的解是全局最优解</li><li>一般情况下，其解不保证是全局最优解</li></ul><h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>凸函数就是一个定义在某个向量空间的凸子集C（区间）上的实值函数 f，而且对于凸子集C中任意两个向量 $x_1$, $x_2$ 有：</p><script type="math/tex; mode=display">f(\frac{x_1+x_2}{2}) \le \frac{f(x_1)+f(x_2)}{2}</script><p>于是容易得出对于任意（0,1)中有理数 p，有：</p><script type="math/tex; mode=display">f(px_1+(1-p)x_2) \le pf(x_1)+(1-p)f(x_2)</script><p>如果 f 连续，那么 p 可以改成任意（0,1）中实数。则 f 称为 I 上的凸函数，当且仅当其上境图（在函数图像上方的点集）为一个凸集。</p><h1 id="梯度下降法的使用"><a href="#梯度下降法的使用" class="headerlink" title="梯度下降法的使用"></a>梯度下降法的使用</h1><p>我们首先在函数上任选一点，计算其损失（即我们上面的L(w)） ，然后按照某一规则寻找更低的一点计算新的损失，只要新损失更小（最小化问题），我们就继续下降，直到达到一个可接受的优化目标。<br>梯度下降方法分为两个部分，第一部分是整体上，我们使用某步长不断下降求损失函数，第二部分是为了防止步长太长导致最后无法收敛，每次当损失上升的时候都调整步长。<br>通常实践中使用时，都是用一些开源算法，很少需要深度改进，比如使用 libsvm 可以直接求解逻辑回归。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p><a href="http://www.cnblogs.com/yysblog/p/3268508.html" target="_blank" rel="noopener">http://www.cnblogs.com/yysblog/p/3268508.html</a><br><a href="http://52opencourse.com/125/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AD%E8%AF%BE-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-logistic-regression" target="_blank" rel="noopener">http://52opencourse.com/125/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AD%E8%AF%BE-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-logistic-regression</a><br><a href="http://www.cnblogs.com/chaoren399/p/4851658.html" target="_blank" rel="noopener">http://www.cnblogs.com/chaoren399/p/4851658.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;本系列的课程来源是 &lt;a href=&quot;https://see.stanford.edu/Course/CS229&quot; target=&quot;_bla
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://luojinping.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Jackson反序列化忽略为null的字段</title>
    <link href="http://luojinping.com/2017/08/19/Jackson%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%BF%BD%E7%95%A5%E4%B8%BAnull%E7%9A%84%E5%AD%97%E6%AE%B5/"/>
    <id>http://luojinping.com/2017/08/19/Jackson反序列化忽略为null的字段/</id>
    <published>2017-08-19T15:02:30.000Z</published>
    <updated>2019-08-25T13:32:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h1><p>json 反序列化 bean 时，当某个字段在 json 中为 null 时，使用 bean 中声明的默认值。</p><p>Person 类我们改造下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="comment">// Address is a enum: &#123;CH, US, GZ&#125;</span></span><br><span class="line">  <span class="keyword">private</span> Region region = Region.GZ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>仍然以 Person 类举例，如果 json 串是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"robert"</span>, <span class="string">"region"</span>:<span class="keyword">null</span>&#125;</span><br></pre></td></tr></table></figure></p><p>希望反序列化后的 bean 为<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person(name=<span class="string">"robert"</span>, region=Region.GZ)</span><br></pre></td></tr></table></figure></p><h1 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h1><p>在上一篇文章 <code>lombok 的 AllArgs 导致 Jackson 反序列化丢失字段默认值</code> 中可以看到 json 反序列化为 bean 的过程，一般情况下，是先调用默认构造函数生成 bean，然后根据 json 中出现的字段挨个赋值。<br>所以反序列化生成的 bean 的 region 肯定为 null。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="JsonInclude-Include-NON-NULL-可行吗？"><a href="#JsonInclude-Include-NON-NULL-可行吗？" class="headerlink" title="@JsonInclude(Include.NON_NULL) 可行吗？"></a>@JsonInclude(Include.NON_NULL) 可行吗？</h2><p>不可行，这个注解是序列化时忽略 null 值，反序列化时不生效，基本上反序列化时我们不能做什么事情。</p><h2 id="JsonCreator-可行吗？"><a href="#JsonCreator-可行吗？" class="headerlink" title="JsonCreator 可行吗？"></a>JsonCreator 可行吗？</h2><p>在 Region 枚举里写 JsonCreator:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@JsonCreator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Region <span class="title">getRegion</span><span class="params">(String value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Region region : Region.values()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (region.name().equals(value)) &#123;</span><br><span class="line">            <span class="keyword">return</span> region;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Region.GZ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>直接将 <code>{&quot;region&quot;: null}</code> 反序列化为 Region 是可行的，会调用 JsonCreator，但是如果是反序列化 Person 则不会调用到 JsonCreator，为什么呢？</p><p>debug 过程：<br>如前文所述，会调用到 <code>com.fasterxml.jackson.databind.deser.BeanDeserializer#deserialize</code> 这个函数中，然后会调用到<br><code>com.fasterxml.jackson.databind.deser.SettableBeanProperty#deserialize</code>，这个函数的实现是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Object <span class="title">deserialize</span><span class="params">(JsonParser p, DeserializationContext ctxt)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    JsonToken t = p.getCurrentToken();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (t == JsonToken.VALUE_NULL) &#123;</span><br><span class="line">        <span class="keyword">return</span> _valueDeserializer.getNullValue(ctxt);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (_valueTypeDeserializer != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> _valueDeserializer.deserializeWithType(p, ctxt, _valueTypeDeserializer);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> _valueDeserializer.deserialize(p, ctxt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以在这里会把 null 值拦住，直接返回 getNullValue 的结果。</p><h2 id="自定义-deserializer"><a href="#自定义-deserializer" class="headerlink" title="自定义 deserializer"></a>自定义 deserializer</h2><p>实现如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RegionDeserializer</span> <span class="keyword">extends</span> <span class="title">JsonDeserializer</span>&lt;<span class="title">Region</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Region <span class="title">deserialize</span><span class="params">(JsonParser jsonParser, DeserializationContext deserializationContext)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        JsonNode node = jsonParser.getCodec().readTree(jsonParser);</span><br><span class="line">        Region region = Region.GZ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtils.isNotEmpty(node.textValue())) &#123;</span><br><span class="line">                <span class="keyword">return</span> Region.getRegion(node.textValue());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            type = Region.GZ;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> region;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Region <span class="title">getNullValue</span><span class="params">(DeserializationContext ctxt)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Region.GZ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Person 类改为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Address is a enum: &#123;CH, US, GZ&#125;</span></span><br><span class="line">  <span class="meta">@JsonDeserialize</span>(using = RegionDeserializer.class)</span><br><span class="line">  <span class="keyword">private</span> Region region = Region.GZ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样，在<code>com.fasterxml.jackson.databind.deser.SettableBeanProperty#deserialize</code>这个方法里，碰到 null 值，就会返回 getNullValue 的结果，即 Region.GZ，如果不是 null 会进入 getRegion 函数处理，也能处理其他情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;要解决的问题&quot;&gt;&lt;a href=&quot;#要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;要解决的问题&quot;&gt;&lt;/a&gt;要解决的问题&lt;/h1&gt;&lt;p&gt;json 反序列化 bean 时，当某个字段在 json 中为 null 时，使用 bean 中声明的默认值
      
    
    </summary>
    
    
      <category term="Java" scheme="http://luojinping.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>lombok的AllArgsConstructor注解导致Jackson反序列化后丢失字段默认值</title>
    <link href="http://luojinping.com/2017/08/19/lombok%E7%9A%84AllArgsConstructor%E6%B3%A8%E8%A7%A3%E5%AF%BC%E8%87%B4Jackson%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%90%8E%E4%B8%A2%E5%A4%B1%E5%AD%97%E6%AE%B5%E9%BB%98%E8%AE%A4%E5%80%BC/"/>
    <id>http://luojinping.com/2017/08/19/lombok的AllArgsConstructor注解导致Jackson反序列化后丢失字段默认值/</id>
    <published>2017-08-19T14:27:12.000Z</published>
    <updated>2019-08-25T13:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h1><p>希望在反序列化 json 到 bean 时，对于 json 中未出现的字段，在 bean 中赋上默认值。</p><p>例如<br>Person 类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@JsonIgnoreProperties</span>(ignoreUnknown = <span class="keyword">true</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> String address = <span class="string">"beijing"</span>; <span class="comment">// default value if json missing the age field</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;robert&quot;&#125;</span><br></pre></td></tr></table></figure></p><p>反序列化后的 bean 为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person(name=&quot;robert&quot;, address=&quot;beijing&quot;)</span><br></pre></td></tr></table></figure></p><p>但实际上，发序列化的结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person(name=&quot;robert&quot;, address=null)</span><br></pre></td></tr></table></figure></p><h1 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h1><h2 id="查看-maven-版本"><a href="#查看-maven-版本" class="headerlink" title="查看 maven 版本"></a>查看 maven 版本</h2><p>项目中 jackson 的配置如下</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Jackson dependency versions --&gt;</span><br><span class="line">&lt;jackson.version&gt;2.6.5&lt;/jackson.version&gt;</span><br></pre></td></tr></table></figure><p>配置升到最新后问题仍然存在。</p><h2 id="debug-json-反序列化过程，找到原因"><a href="#debug-json-反序列化过程，找到原因" class="headerlink" title="debug json 反序列化过程，找到原因"></a>debug json 反序列化过程，找到原因</h2><p>json 反序列化是从<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.fasterxml.jackson.databind.ObjectMapper#_readMapAndClose</span><br></pre></td></tr></table></figure></p><p>这个方法调用开始的，里面的一段代码为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DeserializationConfig cfg = getDeserializationConfig();</span><br><span class="line">DeserializationContext ctxt = createDeserializationContext(jp, cfg);</span><br><span class="line">JsonDeserializer&lt;Object&gt; deser = _findRootDeserializer(ctxt, valueType);</span><br><span class="line"><span class="keyword">if</span> (cfg.useRootWrapping()) &#123;</span><br><span class="line">    result = _unwrapAndDeserialize(jp, ctxt, cfg, valueType, deser);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    result = deser.deserialize(jp, ctxt);</span><br><span class="line">&#125;</span><br><span class="line">ctxt.checkUnresolvedObjectId();</span><br></pre></td></tr></table></figure><p>在第 3 行找到的 JsonDeserializer 是 <code>com.fasterxml.jackson.databind.deser.BeanDeserializer</code><br>从第 7 行代表进入 <code>com.fasterxml.jackson.databind.deser.BeanDeserializer#deserialize(com.fasterxml.jackson.core.JsonParser, com.fasterxml.jackson.databind.DeserializationContext)</code></p><p>函数实现如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">deserialize</span><span class="params">(JsonParser p, DeserializationContext ctxt)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// common case first</span></span><br><span class="line">    <span class="keyword">if</span> (p.isExpectedStartObjectToken()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (_vanillaProcessing) &#123;</span><br><span class="line">            <span class="keyword">return</span> vanillaDeserialize(p, ctxt, p.nextToken());</span><br><span class="line">        &#125;</span><br><span class="line">        p.nextToken();</span><br><span class="line">        <span class="keyword">if</span> (_objectIdReader != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> deserializeWithObjectId(p, ctxt);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> deserializeFromObject(p, ctxt);</span><br><span class="line">    &#125;</span><br><span class="line">    JsonToken t = p.getCurrentToken();</span><br><span class="line">    <span class="keyword">return</span> _deserializeOther(p, ctxt, t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>vanillaDeserialize 为 false，最后走到了第 11 行，最后到了<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.fasterxml.jackson.databind.deser.BeanDeserializer#_deserializeUsingPropertyBased</span><br></pre></td></tr></table></figure></p><p>然后到<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator#build</span><br></pre></td></tr></table></figure></p><p>在这个函数里有这样一段代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Object bean = _valueInstantiator.createFromObjectWith(ctxt, buffer.getParameters(_allProperties));</span><br></pre></td></tr></table></figure></p><p>调用的是<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.fasterxml.jackson.databind.deser.ValueInstantiator#createFromObjectWith(com.fasterxml.jackson.databind.DeserializationContext, java.lang.Object[])</span><br></pre></td></tr></table></figure></p><p>可以发现，createFromObjectWith 的第二个参数是数组，json 解出来的字段都放在了这个数组里。然后调用了 Person 类的全参构造函数，对于<br>缺失的字段自动补 null 值，这样就导致了 address 字段为 null。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>去掉 @AllArgsConstructor 时，没有问题了，因为此时找到的 <code>com.fasterxml.jackson.databind.deser.BeanDeserializer</code> 的  vanillaDeserialize 字段为 true，会调用 <code>vanillaDeserialize(p, ctxt, p.nextToken());</code>，这个函数的实现非常明确：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> Object <span class="title">vanillaDeserialize</span><span class="params">(JsonParser p, DeserializationContext ctxt, JsonToken t)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Object bean = _valueInstantiator.createUsingDefault(ctxt);</span><br><span class="line">        <span class="comment">// [databind#631]: Assign current value, to be accessible by custom serializers</span></span><br><span class="line">        p.setCurrentValue(bean);</span><br><span class="line">        <span class="keyword">if</span> (p.hasTokenId(JsonTokenId.ID_FIELD_NAME)) &#123;</span><br><span class="line">            String propName = p.getCurrentName();</span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">                p.nextToken();</span><br><span class="line">                SettableBeanProperty prop = _beanProperties.find(propName);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (prop != <span class="keyword">null</span>) &#123; <span class="comment">// normal case</span></span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        prop.deserializeAndSet(p, ctxt, bean);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        wrapAndThrow(e, bean, propName, ctxt);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                handleUnknownVanilla(p, ctxt, bean, propName);</span><br><span class="line">            &#125; <span class="keyword">while</span> ((propName = p.nextFieldName()) != <span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>先用默认构造函数生成 bean，此时的 bean 是有默认值的，然后将 json 中出现的字段的值赋值给 bean，这样 address 就有值了。</p><h2 id="根本原因"><a href="#根本原因" class="headerlink" title="根本原因"></a>根本原因</h2><p>看上去是声明了全参构造函数导致的，所以想尝试自己写全参构造函数，在 address 为 null 时给其赋默认值。<br>写完如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, String address)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.address = address;</span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">this</span>.address == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">this</span>.address = <span class="string">"beijing"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>继续走刚才 debug 的流程，发现<strong>居然</strong>没有请求这个全参构造函数。</p><p>那问题就是 @AllArgsConstructor 生成的全参函数有不同之处，jackson 能够识别出来并用于反序列化。查看 jar 包中 Person 类的代码发现其全参构造函数如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ConstructorProperties</span>(&#123;<span class="string">"name"</span>, <span class="string">"address"</span>&#125;)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, String address)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.address = address;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以，区别就是 <code>@ConstructorProperties({&quot;name&quot;, &quot;address&quot;})</code> 这个注解，这个注解的作用是指定构造函数参数的名字，Spring 可根据参数的名字注入 bean。</p><p>(补充自 liwei)<br>jackson 调用了全参构造函数的原因在于@AllArgsConstructor 的构造函数有ConstructorProperties ,jackson在选择构造函数的时候会调用BasicDeserializerFactory._addDeserialzerContructors方法，他首先选择无参构造函数，并遍历所有的构造函数，如果存在具有@ConstructProperties注解的构造函数，则把该构造函数作为默认创建bean的构造函数，如下：<br><img src="/img/jackson_constructor.png" alt></p><p>可以通过设置 @AllArgsConstructor(suppressConstructorProperties=true) 来禁用 @ConstructorProperties.</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>Lombok 的 @AllArgsConstructor 注解导致 Jackson 反序列化时调用了全参构造函数，将没有出现的字段都赋值为 null 了。</p><p>修改方式：</p><ol><li>不使用 @AllArgsConstructor</li><li>使用 @AllArgsConstructor 但是不让其在全参构造函数上加入 ConstructorProperties 注解，声明方式改为 @AllArgsConstructor(suppressConstructorProperties = true)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;要解决的问题&quot;&gt;&lt;a href=&quot;#要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;要解决的问题&quot;&gt;&lt;/a&gt;要解决的问题&lt;/h1&gt;&lt;p&gt;希望在反序列化 json 到 bean 时，对于 json 中未出现的字段，在 bean 中赋上默认值。&lt;/
      
    
    </summary>
    
    
      <category term="Java" scheme="http://luojinping.com/tags/Java/"/>
    
  </entry>
  
</feed>
